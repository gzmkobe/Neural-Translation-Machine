### This is a natural language processing course-based project. The project is to designed an encoder-decoder with multiple attention mechanism to translate Chinese/Vietnamese to English


<pre>
 <b>ch2en_gru.ipynb:</b> Includes GRU encoder and GRU-based Luong attention/no attention model for Chinese to English translation.
</pre>


<pre>
 <b>ch2en_lstm.ipynb:</b> Includes lstm encoder and lstm-based Luong attention/no attention decoder model for Chinese to English translation.
</pre>

<pre>
 <b>vi2en.ipynb:</b> Includes lstm/gru encoder and lstm-based/gru-based Luong attention/no attention decoder model for Vietnamese to English translation.
</pre>

## Completed
:white_check_mark: Add dataloader

:white_check_mark: Train Unknown word representation

:white_check_mark: Mask

:white_check_mark: Minibatch

:white_check_mark: Bleu score

:white_check_mark: Save model

:white_check_mark: Self-attention

:white_check_mark: Multiple layers in encoder and decoder

:white_check_mark: Without Attention

:white_check_mark: LSTM

:white_check_mark: Bean Search

:white_check_mark: Play around the original data with respect to tokenization


## Future Work
* Ipynb to py

* Transformers



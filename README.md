# This is a natural language processing course-based project. The project is to designed an RNN-based encoder and decoder with self-attention mechanism to translate Chinese/Vietnamese to English

<pre>
 <b>ch2en_gru.ipynb:</b> Includes GRU encoder and GRU-based Luong attention/no attention model for Chinese to English translation.
</pre>


<pre>
 <b>ch2en_lstm.ipynb:</b> Includes lstm encoder and lstm-based Luong attention/no attention decoder model for Chinese to English translation.
</pre>

<pre>
 <b>vi2en.ipynb:</b> Includes lstm/gru encoder and lstm-based/gru-based Luong attention/no attention decoder model for Vietnamese to English translation.
</pre>


<pre>
 <b>vi2en.ipynb:</b> Includes lstm/gru encoder and lstm-based/gru-based Luong attention/no attention decoder model for Vietnamese to English translation.
</pre>


<pre>
 <b>vi2en_self.ipynb:</b> Includes self-attention encoder and gru-based decoder model for Vietnamese to English translation.
</pre>



<pre>
 <b>ch2en_self.ipynb:</b> Includes self-attention encoder and gru-based decoder model for Chinese to English translation.
</pre>


## Completed
:white_check_mark: Add dataloader

:white_check_mark: Train Unknown word representation

:white_check_mark: Mask

:white_check_mark: Minibatch

:white_check_mark: Bleu score

:white_check_mark: Save model

:white_check_mark: Self-attention

:white_check_mark: Multiple layers in encoder and decoder

:white_check_mark: Without Attention

:white_check_mark: LSTM

:white_check_mark: Beam Search

:white_check_mark: character-level Chinese

## Future Work
* Ipynb to py

* Transformers

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "from torch.utils.data import Dataset,RandomSampler\n",
    "import operator\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "vocab_size = 85000\n",
    "hidden_size = 256\n",
    "# emb_size = 256\n",
    "MAX_LENGTH_1 = 100 # since 99% source sentence is <= 100\n",
    "# MAX_LENGTH_1 = max(len(pair[0].split(\" \")) for pair in pairs)\n",
    "# MAX_LENGTH_2 = max(len(pair[1].split(\" \")) for pair in pairs)\n",
    "dropout_p = 0.1\n",
    "teacher_forcing_ratio = 0.5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 88919\n",
      "chi 69127\n",
      "Build vocabulary by top 85000 frequent word...\n",
      "eng 85004\n",
      "['我们 有 更多 的 智慧 和 更 强 的 适应 适应性    可以 学到 更多 知识    还 能 在 更多 不同 的 环境 下生 生存    人类 在 地球 各处 居住   甚至 至上 了 外太空 太空  ', 'We &apos;re smarter , we &apos;re more flexible , we can learn more , we survive in more different environments , we migrated to cover the world and even go to outer space .']\n"
     ]
    }
   ],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"PAD\", 1: \"SOS\", 2: \"EOS\",3:\"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        \n",
    "            \n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lang1_lines = open('data/iwslt-zh-en/train.tok.zh', encoding = 'utf-8').read().\\\n",
    "                    strip().split('\\n')\n",
    "    lang2_lines = open('data/iwslt-zh-en/train.tok.en', encoding = 'utf-8').read().\\\n",
    "                    strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [(lang1_lines[i] +',' + lang2_lines[i]).split(',',1) for i in range(len(lang1_lines))]\n",
    "\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def build_topwordVocab(lang, vocab_size):\n",
    "    print(\"Build vocabulary by top {} frequent word...\".format(vocab_size))\n",
    "    sorted_word2Count = sorted(lang.word2count.items(),\n",
    "        key=operator.itemgetter(1),\n",
    "        reverse=True)\n",
    "    sorted_words = [x[0] for x in sorted_word2Count[:vocab_size]]\n",
    "    \n",
    "    \n",
    "    lang.index2word = {}\n",
    "    lang.index2word[0] = \"PAD\"\n",
    "    lang.index2word[1] = \"SOS\"\n",
    "    lang.index2word[2] = \"EOS\"\n",
    "    lang.index2word[3] = \"UNK\"\n",
    "    \n",
    "    for ind, word in enumerate(sorted_words):\n",
    "            lang.index2word[ind + 4] = word\n",
    "            \n",
    "\n",
    "    lang.word2index = {}\n",
    "    for ind, word in enumerate(sorted_words):\n",
    "        lang.word2index[word] = ind + 4\n",
    "    \n",
    "    lang.n_words = len(lang.index2word)\n",
    "    \n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'chi')\n",
    "\n",
    "input_lang = build_topwordVocab(input_lang,vocab_size)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_word2Count = sorted(input_lang.word2count.items(),\n",
    "    key=operator.itemgetter(1),\n",
    "    reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_word2Count ###标点符号排第一 之后要改掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            idxs.append(lang.word2index[word])\n",
    "        except KeyError:\n",
    "            idxs.append(3)  # 3 is the id of 'UNK'\n",
    "    return idxs\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "#         pairs = [tensorsFromPair(pair) for pair in pairs]\n",
    "#         self.source_sent_list = [i[0] for i in pairs]\n",
    "#         self.target_sent_list = [i[1] for i in pairs]\n",
    "\n",
    "        self.source_sent_list = [indexesFromSentence(input_lang,pair[0]) for pair in pairs]\n",
    "        self.target_sent_list = [indexesFromSentence(output_lang,pair[1]) for pair in pairs]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_sent_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        token1_idx = self.source_sent_list[key][:MAX_LENGTH_1]\n",
    "        token2_idx = self.target_sent_list[key][:MAX_LENGTH_1]\n",
    "        return [token1_idx,token2_idx, len(token1_idx), len(token2_idx)]\n",
    "\n",
    "    \n",
    "def Vocab_collate_func(batch):\n",
    "    source_sent_list = []\n",
    "    target_sent_list = []\n",
    "    source_len_list = []\n",
    "    target_len_list = []\n",
    "\n",
    "    for datum in batch:   ### batch = sample\n",
    "        source_len_list.append(datum[2])\n",
    "        target_len_list.append(datum[3])\n",
    "\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        \n",
    "        # source sentence processing\n",
    "        padded_source = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH_1-datum[2])),          ### 0代表左边没有pad,右边的值代表右边pad的个数\n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        source_sent_list.append(padded_source)\n",
    "        \n",
    "        # target sentence processing\n",
    "        padded_target = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_LENGTH_1-datum[3])),          ### 0代表左边没有pad,右边的值代表右边pad的个数\n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        target_sent_list.append(padded_target)\n",
    "        \n",
    "    return [torch.tensor(source_sent_list,device = device), \n",
    "            torch.tensor(target_sent_list,device = device),\n",
    "            torch.LongTensor(source_len_list,device = device), \n",
    "            torch.LongTensor(target_len_list,device = device)]\n",
    "\n",
    "train_dataset = VocabDataset(pairs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=Vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True) # embedding size = hidden size\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "    def initHidden(self,BATCH_SIZE):\n",
    "        return torch.zeros(2, BATCH_SIZE, self.hidden_size, device=device) # return (2,1,hidden_size) 2 due to bidirection\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, BATCH_SIZE, -1)  # input is just one token at timpstep t\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)  \n",
    "        # (seq_len, batch, num_directions * hidden_size) and (num_layers * num_directions, batch, hidden_size)\n",
    "        output = self.fc1(output)\n",
    "        return output, hidden\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH_1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, BATCH_SIZE, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "#         embedded: torch.Size([1, 32, 256])\n",
    "#         hidden: torch.Size([1, 32, 256])\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)   \n",
    "#         attn_weights:torch.Size([32, 100])\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs.transpose(0,1))\n",
    "#         encoder_outputs: 100*32*512 attn_applied: 32*1*512\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied.transpose(0,1)[0]), 1)\n",
    "        # output: 32*768\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # output 1*32*256\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         output: torch.Size([32, 69126])\n",
    "#         hidden: torch.Size([1, 32, 256])\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is just one sentence input, could be batchlized \n",
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion, mask = None):\n",
    "    encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
    "    encoder_optimizer.zero_grad()  # zero out the accumulated gradient over mini-batch\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0) # length of source sentence\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(target_length, BATCH_SIZE, encoder.hidden_size, device=device) # (seq_length, BATCH_SIZE,hidden_size*2) 2 due to bidirection\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # feed-forward layer resulting encoder outputs, ei refers to each word token in input sentence\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)  \n",
    "        # encoder_output: torch.Size([1, 32, 512]) encoder_hidden: torch.Size([2, 32, 256])\n",
    "        encoder_outputs[ei] = encoder_output[0] \n",
    "    # change the shape of encoder output to fit into decoder \n",
    "    encoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "        torch.cat((encoder_hidden[0],encoder_hidden[1]),dim = 1)).unsqueeze(0)\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]*32], device=device)  # decoder_input: torch.Size([1, 32])\n",
    "    # init decoder hidden \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "            loss += temp_loss * mask[di:di+1].float()  \n",
    "            ave_loss = loss.sum()/BATCH_SIZE \n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # decoder_input: torch.Size([1, 32])\n",
    "            # decoder_hidden: torch.Size([1, 32, 256]) 1 token * batch * hidden size\n",
    "            # encoder_outputs: torch.Size([100, 32, 512])\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            # topv: 32*1\n",
    "            # topi: 32*1\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            # decoder_input: 32\n",
    "            # target_tensor: 100*32\n",
    "            # decoder_output: 32*69127 \n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "            loss += temp_loss * mask[di:di+1].float()\n",
    "            # loss size 1*32\n",
    "            ave_loss = loss.sum()/BATCH_SIZE  \n",
    "            \n",
    "    ave_loss.backward()\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.step()   # update parameters\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ave_loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(reduce = False) ##!!!!!!!!!!1 这个loss是否要换成crossentropy\n",
    "\n",
    "    for epoch in range(1, n_iters + 1):\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        for i, (input_sentences, target_sentences,len1,len2) in enumerate(train_loader): \n",
    "            input_tensor = input_sentences.transpose(0,1)   # 32*100 to 100*32\n",
    "            target_tensor = target_sentences.transpose(0,1)\n",
    "            mask = target_tensor.ge(1)   # 100 * 32\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion, mask = mask)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i > 0 and i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}'.format(\n",
    "                    timeSince(start, i + 1/len(train_loader)), epoch, n_iters, i, \n",
    "                    len(train_loader),print_loss_avg))\n",
    "\n",
    "            if i > 0 and i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "        print(plot_losses)\n",
    "        showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH_1):\n",
    "#     with torch.no_grad():\n",
    "#         input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "#         input_length = input_tensor.size()[0]\n",
    "#         encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
    "\n",
    "#         encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
    "\n",
    "#         for ei in range(input_length):\n",
    "#             encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "#                                                      encoder_hidden)\n",
    "#             encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "#         decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        \n",
    "#         decoder_hidden = encoder_hidden\n",
    "\n",
    "#         decoded_words = []\n",
    "#         decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "#         for di in range(max_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             decoder_attentions[di] = decoder_attention.data\n",
    "#             topv, topi = decoder_output.data.topk(1)\n",
    "#             if topi.item() == EOS_token:\n",
    "#                 decoded_words.append('<EOS>')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "#             decoder_input = topi.squeeze().detach()\n",
    "\n",
    "#         return decoded_words, decoder_attentions[:di + 1]\n",
    "    \n",
    "# def evaluateRandomly(encoder, decoder, n=10):\n",
    "#     for i in range(n):\n",
    "#         pair = random.choice(pairs)\n",
    "#         print('>', pair[0])\n",
    "#         print('=', pair[1])\n",
    "#         output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "#         output_sentence = ' '.join(output_words)\n",
    "#         print('<', output_sentence)\n",
    "#         print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0m 54s (- -1m 59s), Epoch: [1/3], Step: [1/6668], Train Loss: 3.5373974609375\n",
      "Time: 1m 15s (- -1m 22s), Epoch: [1/3], Step: [2/6668], Train Loss: 1.6213475036621094\n",
      "Time: 1m 42s (- -2m 51s), Epoch: [1/3], Step: [3/6668], Train Loss: 1.451114501953125\n",
      "Time: 2m 8s (- -2m 23s), Epoch: [1/3], Step: [4/6668], Train Loss: 1.608426055908203\n",
      "Time: 2m 31s (- -3m 58s), Epoch: [1/3], Step: [5/6668], Train Loss: 1.4026364135742186\n",
      "Time: 2m 56s (- -3m 33s), Epoch: [1/3], Step: [6/6668], Train Loss: 1.6417161560058593\n",
      "Time: 3m 22s (- -3m 6s), Epoch: [1/3], Step: [7/6668], Train Loss: 2.0729544067382815\n",
      "Time: 3m 45s (- -4m 42s), Epoch: [1/3], Step: [8/6668], Train Loss: 1.338889617919922\n",
      "Time: 4m 4s (- -4m 22s), Epoch: [1/3], Step: [9/6668], Train Loss: 1.7672528076171874\n",
      "Time: 4m 24s (- -4m 1s), Epoch: [1/3], Step: [10/6668], Train Loss: 1.6693408203125\n",
      "Time: 4m 46s (- -5m 39s), Epoch: [1/3], Step: [11/6668], Train Loss: 1.6399649047851563\n",
      "Time: 5m 22s (- -5m 4s), Epoch: [1/3], Step: [12/6668], Train Loss: 2.4121553039550783\n",
      "Time: 5m 55s (- -6m 31s), Epoch: [1/3], Step: [13/6668], Train Loss: 1.7769537353515625\n",
      "Time: 6m 23s (- -6m 3s), Epoch: [1/3], Step: [14/6668], Train Loss: 1.6234689331054688\n",
      "Time: 6m 45s (- -7m 41s), Epoch: [1/3], Step: [15/6668], Train Loss: 1.788306427001953\n",
      "Time: 7m 7s (- -7m 19s), Epoch: [1/3], Step: [16/6668], Train Loss: 1.892185516357422\n",
      "Time: 7m 29s (- -8m 56s), Epoch: [1/3], Step: [17/6668], Train Loss: 1.6844670104980468\n",
      "Time: 7m 54s (- -8m 32s), Epoch: [1/3], Step: [18/6668], Train Loss: 1.8819163513183594\n",
      "Time: 8m 14s (- -8m 11s), Epoch: [1/3], Step: [19/6668], Train Loss: 1.4029270935058593\n",
      "Time: 8m 35s (- -9m 50s), Epoch: [1/3], Step: [20/6668], Train Loss: 1.7627401733398438\n",
      "Time: 8m 57s (- -9m 27s), Epoch: [1/3], Step: [21/6668], Train Loss: 1.443047332763672\n",
      "Time: 9m 18s (- -9m 6s), Epoch: [1/3], Step: [22/6668], Train Loss: 2.2847508239746093\n",
      "Time: 9m 39s (- -10m 45s), Epoch: [1/3], Step: [23/6668], Train Loss: 1.7148606872558594\n",
      "Time: 9m 58s (- -10m 26s), Epoch: [1/3], Step: [24/6668], Train Loss: 1.4421627807617188\n",
      "Time: 10m 20s (- -10m 3s), Epoch: [1/3], Step: [25/6668], Train Loss: 1.7261227416992186\n",
      "Time: 10m 44s (- -11m 40s), Epoch: [1/3], Step: [26/6668], Train Loss: 1.6585087585449219\n",
      "Time: 11m 4s (- -11m 20s), Epoch: [1/3], Step: [27/6668], Train Loss: 1.5600987243652344\n",
      "Time: 11m 25s (- -12m 59s), Epoch: [1/3], Step: [28/6668], Train Loss: 1.8995732116699218\n",
      "Time: 11m 47s (- -12m 37s), Epoch: [1/3], Step: [29/6668], Train Loss: 1.8414283752441407\n",
      "Time: 12m 8s (- -12m 15s), Epoch: [1/3], Step: [30/6668], Train Loss: 1.6849014282226562\n",
      "Time: 12m 29s (- -13m 54s), Epoch: [1/3], Step: [31/6668], Train Loss: 1.5751206970214844\n",
      "Time: 12m 50s (- -13m 33s), Epoch: [1/3], Step: [32/6668], Train Loss: 1.5591908264160157\n",
      "Time: 13m 14s (- -13m 10s), Epoch: [1/3], Step: [33/6668], Train Loss: 1.6343559265136718\n",
      "Time: 13m 37s (- -14m 46s), Epoch: [1/3], Step: [34/6668], Train Loss: 1.48075439453125\n",
      "Time: 13m 58s (- -14m 25s), Epoch: [1/3], Step: [35/6668], Train Loss: 1.777407684326172\n",
      "Time: 14m 19s (- -14m 3s), Epoch: [1/3], Step: [36/6668], Train Loss: 2.00791259765625\n",
      "Time: 14m 40s (- -15m 43s), Epoch: [1/3], Step: [37/6668], Train Loss: 1.6845211791992187\n",
      "Time: 14m 59s (- -15m 24s), Epoch: [1/3], Step: [38/6668], Train Loss: 1.6618362426757813\n",
      "Time: 15m 18s (- -15m 4s), Epoch: [1/3], Step: [39/6668], Train Loss: 1.2329808807373046\n",
      "Time: 15m 40s (- -16m 43s), Epoch: [1/3], Step: [40/6668], Train Loss: 1.8453768920898437\n",
      "Time: 16m 5s (- -16m 18s), Epoch: [1/3], Step: [41/6668], Train Loss: 1.6300752258300781\n",
      "Time: 16m 27s (- -17m 56s), Epoch: [1/3], Step: [42/6668], Train Loss: 1.6447576904296874\n",
      "Time: 16m 46s (- -17m 37s), Epoch: [1/3], Step: [43/6668], Train Loss: 1.8017263793945313\n",
      "Time: 17m 6s (- -17m 16s), Epoch: [1/3], Step: [44/6668], Train Loss: 1.4339315795898437\n",
      "Time: 17m 26s (- -18m 57s), Epoch: [1/3], Step: [45/6668], Train Loss: 1.4996342468261719\n",
      "Time: 17m 47s (- -18m 35s), Epoch: [1/3], Step: [46/6668], Train Loss: 1.9350381469726563\n",
      "Time: 18m 9s (- -18m 13s), Epoch: [1/3], Step: [47/6668], Train Loss: 1.5474453735351563\n",
      "Time: 18m 30s (- -19m 52s), Epoch: [1/3], Step: [48/6668], Train Loss: 1.8117721557617188\n",
      "Time: 18m 50s (- -19m 32s), Epoch: [1/3], Step: [49/6668], Train Loss: 1.2184051513671874\n",
      "Time: 19m 9s (- -19m 13s), Epoch: [1/3], Step: [50/6668], Train Loss: 1.6918043518066406\n",
      "Time: 19m 36s (- -20m 46s), Epoch: [1/3], Step: [51/6668], Train Loss: 1.8369998168945312\n",
      "Time: 19m 59s (- -20m 23s), Epoch: [1/3], Step: [52/6668], Train Loss: 1.9537002563476562\n",
      "Time: 20m 28s (- -21m 54s), Epoch: [1/3], Step: [53/6668], Train Loss: 1.6039317321777344\n",
      "Time: 20m 50s (- -21m 32s), Epoch: [1/3], Step: [54/6668], Train Loss: 1.617602081298828\n",
      "Time: 21m 17s (- -21m 6s), Epoch: [1/3], Step: [55/6668], Train Loss: 1.2954238891601562\n",
      "Time: 21m 42s (- -22m 40s), Epoch: [1/3], Step: [56/6668], Train Loss: 1.5459187316894532\n",
      "Time: 22m 5s (- -22m 17s), Epoch: [1/3], Step: [57/6668], Train Loss: 1.7721243286132813\n",
      "Time: 22m 29s (- -23m 53s), Epoch: [1/3], Step: [58/6668], Train Loss: 1.6733743286132812\n",
      "Time: 22m 50s (- -23m 32s), Epoch: [1/3], Step: [59/6668], Train Loss: 1.80334228515625\n",
      "Time: 23m 12s (- -23m 10s), Epoch: [1/3], Step: [60/6668], Train Loss: 2.031373291015625\n",
      "Time: 23m 36s (- -24m 46s), Epoch: [1/3], Step: [61/6668], Train Loss: 1.4929301452636718\n",
      "Time: 23m 55s (- -24m 27s), Epoch: [1/3], Step: [62/6668], Train Loss: 1.6382994079589843\n",
      "Time: 24m 17s (- -24m 5s), Epoch: [1/3], Step: [63/6668], Train Loss: 1.9106936645507813\n",
      "Time: 24m 37s (- -25m 45s), Epoch: [1/3], Step: [64/6668], Train Loss: 1.3141542053222657\n",
      "Time: 24m 58s (- -25m 24s), Epoch: [1/3], Step: [65/6668], Train Loss: 1.72520263671875\n",
      "Time: 25m 17s (- -25m 5s), Epoch: [1/3], Step: [66/6668], Train Loss: 1.3292298889160157\n",
      "Time: 25m 39s (- -26m 43s), Epoch: [1/3], Step: [67/6668], Train Loss: 1.771908721923828\n",
      "Time: 26m 0s (- -26m 22s), Epoch: [1/3], Step: [68/6668], Train Loss: 1.421539306640625\n",
      "Time: 26m 19s (- -26m 2s), Epoch: [1/3], Step: [69/6668], Train Loss: 1.5188580322265626\n",
      "Time: 26m 49s (- -27m 33s), Epoch: [1/3], Step: [70/6668], Train Loss: 1.2999595642089843\n",
      "Time: 27m 12s (- -27m 10s), Epoch: [1/3], Step: [71/6668], Train Loss: 1.7652571105957031\n",
      "Time: 27m 35s (- -28m 47s), Epoch: [1/3], Step: [72/6668], Train Loss: 1.7863456726074218\n",
      "Time: 27m 56s (- -28m 26s), Epoch: [1/3], Step: [73/6668], Train Loss: 1.3375982666015624\n",
      "Time: 28m 16s (- -28m 6s), Epoch: [1/3], Step: [74/6668], Train Loss: 1.3969291687011718\n",
      "Time: 28m 36s (- -29m 46s), Epoch: [1/3], Step: [75/6668], Train Loss: 1.5221142578125\n",
      "Time: 28m 54s (- -29m 28s), Epoch: [1/3], Step: [76/6668], Train Loss: 1.4025442504882812\n",
      "Time: 29m 11s (- -29m 10s), Epoch: [1/3], Step: [77/6668], Train Loss: 1.455523223876953\n",
      "Time: 29m 31s (- -30m 51s), Epoch: [1/3], Step: [78/6668], Train Loss: 1.67591796875\n",
      "Time: 29m 51s (- -30m 31s), Epoch: [1/3], Step: [79/6668], Train Loss: 1.724061279296875\n",
      "Time: 30m 9s (- -30m 13s), Epoch: [1/3], Step: [80/6668], Train Loss: 1.7159512329101563\n",
      "Time: 30m 27s (- -31m 55s), Epoch: [1/3], Step: [81/6668], Train Loss: 1.522364959716797\n",
      "Time: 30m 48s (- -31m 33s), Epoch: [1/3], Step: [82/6668], Train Loss: 1.4209828186035156\n",
      "Time: 31m 6s (- -31m 16s), Epoch: [1/3], Step: [83/6668], Train Loss: 1.8812861633300781\n",
      "Time: 31m 26s (- -32m 56s), Epoch: [1/3], Step: [84/6668], Train Loss: 1.34815185546875\n",
      "Time: 31m 43s (- -32m 38s), Epoch: [1/3], Step: [85/6668], Train Loss: 1.3789784240722656\n",
      "Time: 32m 3s (- -32m 19s), Epoch: [1/3], Step: [86/6668], Train Loss: 1.573816680908203\n",
      "Time: 32m 22s (- -33m 59s), Epoch: [1/3], Step: [87/6668], Train Loss: 1.6879127502441407\n",
      "Time: 32m 40s (- -33m 41s), Epoch: [1/3], Step: [88/6668], Train Loss: 1.5401896667480468\n",
      "Time: 33m 0s (- -33m 22s), Epoch: [1/3], Step: [89/6668], Train Loss: 1.7125022888183594\n",
      "Time: 33m 17s (- -33m 4s), Epoch: [1/3], Step: [90/6668], Train Loss: 1.9432374572753905\n",
      "Time: 33m 35s (- -34m 46s), Epoch: [1/3], Step: [91/6668], Train Loss: 1.7868301391601562\n",
      "Time: 33m 55s (- -34m 26s), Epoch: [1/3], Step: [92/6668], Train Loss: 1.4036483764648438\n",
      "Time: 34m 13s (- -34m 8s), Epoch: [1/3], Step: [93/6668], Train Loss: 1.8573031616210938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 34m 32s (- -35m 49s), Epoch: [1/3], Step: [94/6668], Train Loss: 1.4132022094726562\n",
      "Time: 34m 50s (- -35m 31s), Epoch: [1/3], Step: [95/6668], Train Loss: 1.300406494140625\n",
      "Time: 35m 9s (- -35m 12s), Epoch: [1/3], Step: [96/6668], Train Loss: 1.3192240905761718\n",
      "Time: 35m 30s (- -36m 51s), Epoch: [1/3], Step: [97/6668], Train Loss: 1.7629730224609375\n",
      "Time: 35m 48s (- -36m 33s), Epoch: [1/3], Step: [98/6668], Train Loss: 1.6520623779296875\n",
      "Time: 36m 7s (- -36m 14s), Epoch: [1/3], Step: [99/6668], Train Loss: 1.5467344665527343\n",
      "Time: 36m 25s (- -37m 56s), Epoch: [1/3], Step: [100/6668], Train Loss: 1.6968312072753906\n",
      "Time: 36m 43s (- -37m 37s), Epoch: [1/3], Step: [101/6668], Train Loss: 1.4376473999023438\n",
      "Time: 37m 3s (- -37m 18s), Epoch: [1/3], Step: [102/6668], Train Loss: 1.3995938110351562\n",
      "Time: 37m 21s (- -37m 0s), Epoch: [1/3], Step: [103/6668], Train Loss: 1.3905220031738281\n",
      "Time: 37m 38s (- -38m 42s), Epoch: [1/3], Step: [104/6668], Train Loss: 1.6971363830566406\n",
      "Time: 37m 58s (- -38m 23s), Epoch: [1/3], Step: [105/6668], Train Loss: 1.496309814453125\n",
      "Time: 38m 18s (- -38m 3s), Epoch: [1/3], Step: [106/6668], Train Loss: 1.5658132934570312\n",
      "Time: 38m 38s (- -39m 42s), Epoch: [1/3], Step: [107/6668], Train Loss: 1.714431610107422\n",
      "Time: 38m 59s (- -39m 22s), Epoch: [1/3], Step: [108/6668], Train Loss: 1.4396060180664063\n",
      "Time: 39m 18s (- -39m 2s), Epoch: [1/3], Step: [109/6668], Train Loss: 1.39701416015625\n",
      "Time: 39m 37s (- -40m 44s), Epoch: [1/3], Step: [110/6668], Train Loss: 1.5758076477050782\n",
      "Time: 39m 55s (- -40m 26s), Epoch: [1/3], Step: [111/6668], Train Loss: 1.7751473999023437\n",
      "Time: 40m 15s (- -40m 6s), Epoch: [1/3], Step: [112/6668], Train Loss: 1.3352357482910155\n",
      "Time: 40m 32s (- -41m 48s), Epoch: [1/3], Step: [113/6668], Train Loss: 1.6572125244140625\n",
      "Time: 40m 52s (- -41m 29s), Epoch: [1/3], Step: [114/6668], Train Loss: 1.8268492126464844\n",
      "Time: 41m 9s (- -41m 11s), Epoch: [1/3], Step: [115/6668], Train Loss: 1.4966412353515626\n",
      "Time: 41m 27s (- -42m 53s), Epoch: [1/3], Step: [116/6668], Train Loss: 1.68262451171875\n",
      "Time: 41m 47s (- -42m 33s), Epoch: [1/3], Step: [117/6668], Train Loss: 1.6192231750488282\n",
      "Time: 42m 5s (- -42m 16s), Epoch: [1/3], Step: [118/6668], Train Loss: 1.413600616455078\n",
      "Time: 42m 24s (- -43m 56s), Epoch: [1/3], Step: [119/6668], Train Loss: 1.5054289245605468\n",
      "Time: 42m 42s (- -43m 39s), Epoch: [1/3], Step: [120/6668], Train Loss: 1.5621763610839843\n",
      "Time: 43m 1s (- -43m 19s), Epoch: [1/3], Step: [121/6668], Train Loss: 1.6071232604980468\n",
      "Time: 43m 21s (- -44m 59s), Epoch: [1/3], Step: [122/6668], Train Loss: 1.7078251647949219\n",
      "Time: 43m 41s (- -44m 39s), Epoch: [1/3], Step: [123/6668], Train Loss: 1.5685183715820312\n",
      "Time: 44m 1s (- -44m 20s), Epoch: [1/3], Step: [124/6668], Train Loss: 1.8147659301757812\n",
      "Time: 44m 18s (- -44m 2s), Epoch: [1/3], Step: [125/6668], Train Loss: 2.2623020935058595\n",
      "Time: 44m 36s (- -45m 44s), Epoch: [1/3], Step: [126/6668], Train Loss: 1.9969300842285156\n",
      "Time: 44m 55s (- -45m 25s), Epoch: [1/3], Step: [127/6668], Train Loss: 1.5186897277832032\n",
      "Time: 45m 15s (- -45m 5s), Epoch: [1/3], Step: [128/6668], Train Loss: 1.4795622253417968\n",
      "Time: 45m 33s (- -46m 47s), Epoch: [1/3], Step: [129/6668], Train Loss: 1.9791871643066405\n",
      "Time: 45m 54s (- -46m 26s), Epoch: [1/3], Step: [130/6668], Train Loss: 1.5851925659179686\n",
      "Time: 46m 13s (- -46m 7s), Epoch: [1/3], Step: [131/6668], Train Loss: 1.631085662841797\n",
      "Time: 46m 33s (- -47m 48s), Epoch: [1/3], Step: [132/6668], Train Loss: 1.3902055358886718\n",
      "Time: 46m 52s (- -47m 28s), Epoch: [1/3], Step: [133/6668], Train Loss: 1.5659095764160156\n",
      "Time: 47m 12s (- -47m 8s), Epoch: [1/3], Step: [134/6668], Train Loss: 1.7645413208007812\n",
      "Time: 47m 33s (- -48m 48s), Epoch: [1/3], Step: [135/6668], Train Loss: 1.5242446899414062\n",
      "Time: 47m 50s (- -48m 30s), Epoch: [1/3], Step: [136/6668], Train Loss: 1.252200927734375\n",
      "Time: 48m 8s (- -48m 12s), Epoch: [1/3], Step: [137/6668], Train Loss: 1.6436224365234375\n",
      "Time: 48m 25s (- -49m 55s), Epoch: [1/3], Step: [138/6668], Train Loss: 1.566507568359375\n",
      "Time: 48m 45s (- -49m 35s), Epoch: [1/3], Step: [139/6668], Train Loss: 1.9407145690917968\n",
      "Time: 49m 2s (- -49m 18s), Epoch: [1/3], Step: [140/6668], Train Loss: 1.70338623046875\n",
      "Time: 49m 22s (- -50m 58s), Epoch: [1/3], Step: [141/6668], Train Loss: 1.4812681579589844\n",
      "Time: 49m 42s (- -50m 38s), Epoch: [1/3], Step: [142/6668], Train Loss: 1.554951171875\n",
      "Time: 50m 1s (- -50m 19s), Epoch: [1/3], Step: [143/6668], Train Loss: 1.822157745361328\n",
      "Time: 50m 19s (- -50m 1s), Epoch: [1/3], Step: [144/6668], Train Loss: 1.830059814453125\n",
      "Time: 50m 39s (- -51m 41s), Epoch: [1/3], Step: [145/6668], Train Loss: 1.7094142150878906\n",
      "Time: 50m 56s (- -51m 24s), Epoch: [1/3], Step: [146/6668], Train Loss: 1.8428132629394531\n",
      "Time: 51m 14s (- -51m 6s), Epoch: [1/3], Step: [147/6668], Train Loss: 1.8328501892089843\n",
      "Time: 51m 32s (- -52m 48s), Epoch: [1/3], Step: [148/6668], Train Loss: 1.4821519470214843\n",
      "Time: 51m 49s (- -52m 31s), Epoch: [1/3], Step: [149/6668], Train Loss: 1.7649339294433595\n",
      "Time: 52m 9s (- -52m 11s), Epoch: [1/3], Step: [150/6668], Train Loss: 1.94272216796875\n",
      "Time: 52m 26s (- -53m 53s), Epoch: [1/3], Step: [151/6668], Train Loss: 1.607965087890625\n",
      "Time: 52m 44s (- -53m 36s), Epoch: [1/3], Step: [152/6668], Train Loss: 1.5572587585449218\n",
      "Time: 53m 3s (- -53m 17s), Epoch: [1/3], Step: [153/6668], Train Loss: 1.4414266967773437\n",
      "Time: 53m 23s (- -54m 57s), Epoch: [1/3], Step: [154/6668], Train Loss: 1.6270880126953124\n",
      "Time: 53m 43s (- -54m 37s), Epoch: [1/3], Step: [155/6668], Train Loss: 1.7219340515136718\n",
      "Time: 54m 3s (- -54m 17s), Epoch: [1/3], Step: [156/6668], Train Loss: 1.3513572692871094\n",
      "Time: 54m 22s (- -55m 58s), Epoch: [1/3], Step: [157/6668], Train Loss: 2.2375167846679687\n",
      "Time: 54m 40s (- -55m 40s), Epoch: [1/3], Step: [158/6668], Train Loss: 1.7420980834960937\n",
      "Time: 54m 57s (- -55m 23s), Epoch: [1/3], Step: [159/6668], Train Loss: 1.7981588745117187\n",
      "Time: 55m 17s (- -55m 3s), Epoch: [1/3], Step: [160/6668], Train Loss: 1.6831675720214845\n",
      "Time: 55m 34s (- -56m 45s), Epoch: [1/3], Step: [161/6668], Train Loss: 1.5849928283691406\n",
      "Time: 55m 54s (- -56m 26s), Epoch: [1/3], Step: [162/6668], Train Loss: 1.7190216064453125\n",
      "Time: 56m 14s (- -56m 6s), Epoch: [1/3], Step: [163/6668], Train Loss: 1.9907017517089844\n",
      "Time: 56m 31s (- -57m 48s), Epoch: [1/3], Step: [164/6668], Train Loss: 1.22275634765625\n",
      "Time: 56m 49s (- -57m 31s), Epoch: [1/3], Step: [165/6668], Train Loss: 1.294989776611328\n",
      "Time: 57m 8s (- -57m 11s), Epoch: [1/3], Step: [166/6668], Train Loss: 2.1145411682128907\n",
      "Time: 57m 26s (- -58m 54s), Epoch: [1/3], Step: [167/6668], Train Loss: 1.7412837219238282\n",
      "Time: 57m 46s (- -58m 34s), Epoch: [1/3], Step: [168/6668], Train Loss: 1.7531646728515624\n",
      "Time: 58m 5s (- -58m 14s), Epoch: [1/3], Step: [169/6668], Train Loss: 1.496352996826172\n",
      "Time: 58m 27s (- -59m 53s), Epoch: [1/3], Step: [170/6668], Train Loss: 1.9510821533203124\n",
      "Time: 58m 49s (- -59m 31s), Epoch: [1/3], Step: [171/6668], Train Loss: 1.7234642028808593\n",
      "Time: 59m 7s (- -59m 13s), Epoch: [1/3], Step: [172/6668], Train Loss: 1.3100531005859375\n",
      "Time: 59m 25s (- -60m 55s), Epoch: [1/3], Step: [173/6668], Train Loss: 1.5614689636230468\n",
      "Time: 59m 42s (- -60m 37s), Epoch: [1/3], Step: [174/6668], Train Loss: 1.7568798828125\n",
      "Time: 60m 0s (- -60m 20s), Epoch: [1/3], Step: [175/6668], Train Loss: 1.9714718627929688\n",
      "Time: 60m 19s (- -60m 1s), Epoch: [1/3], Step: [176/6668], Train Loss: 1.6103892517089844\n",
      "Time: 60m 38s (- -61m 41s), Epoch: [1/3], Step: [177/6668], Train Loss: 1.4709075927734374\n",
      "Time: 60m 57s (- -61m 22s), Epoch: [1/3], Step: [178/6668], Train Loss: 1.307642364501953\n",
      "Time: 61m 17s (- -61m 2s), Epoch: [1/3], Step: [179/6668], Train Loss: 1.36382568359375\n",
      "Time: 61m 35s (- -62m 45s), Epoch: [1/3], Step: [180/6668], Train Loss: 1.444157257080078\n",
      "Time: 61m 54s (- -62m 25s), Epoch: [1/3], Step: [181/6668], Train Loss: 1.975145263671875\n",
      "Time: 62m 14s (- -62m 6s), Epoch: [1/3], Step: [182/6668], Train Loss: 2.0290509033203126\n",
      "Time: 62m 34s (- -63m 46s), Epoch: [1/3], Step: [183/6668], Train Loss: 1.583321075439453\n",
      "Time: 62m 54s (- -63m 26s), Epoch: [1/3], Step: [184/6668], Train Loss: 1.7227078247070313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63m 13s (- -63m 7s), Epoch: [1/3], Step: [185/6668], Train Loss: 1.8678671264648437\n",
      "Time: 63m 32s (- -64m 47s), Epoch: [1/3], Step: [186/6668], Train Loss: 1.4702879333496093\n",
      "Time: 63m 50s (- -64m 30s), Epoch: [1/3], Step: [187/6668], Train Loss: 1.7260762023925782\n",
      "Time: 64m 9s (- -64m 10s), Epoch: [1/3], Step: [188/6668], Train Loss: 1.4184539794921875\n",
      "Time: 64m 27s (- -65m 52s), Epoch: [1/3], Step: [189/6668], Train Loss: 1.699876251220703\n",
      "Time: 64m 45s (- -65m 35s), Epoch: [1/3], Step: [190/6668], Train Loss: 1.6978616333007812\n",
      "Time: 65m 4s (- -65m 15s), Epoch: [1/3], Step: [191/6668], Train Loss: 1.7648036193847656\n",
      "Time: 65m 24s (- -66m 56s), Epoch: [1/3], Step: [192/6668], Train Loss: 1.9315994262695313\n",
      "Time: 65m 41s (- -66m 38s), Epoch: [1/3], Step: [193/6668], Train Loss: 1.6023661804199218\n",
      "Time: 66m 1s (- -66m 18s), Epoch: [1/3], Step: [194/6668], Train Loss: 2.120616760253906\n",
      "Time: 66m 20s (- -67m 59s), Epoch: [1/3], Step: [195/6668], Train Loss: 1.9595951843261719\n",
      "Time: 66m 38s (- -67m 41s), Epoch: [1/3], Step: [196/6668], Train Loss: 1.5470411682128906\n",
      "Time: 66m 56s (- -67m 24s), Epoch: [1/3], Step: [197/6668], Train Loss: 1.4085177612304687\n",
      "Time: 67m 15s (- -67m 4s), Epoch: [1/3], Step: [198/6668], Train Loss: 1.7678129577636719\n",
      "Time: 67m 33s (- -68m 47s), Epoch: [1/3], Step: [199/6668], Train Loss: 1.5706527709960938\n",
      "Time: 67m 50s (- -68m 29s), Epoch: [1/3], Step: [200/6668], Train Loss: 1.8140953063964844\n",
      "Time: 68m 8s (- -68m 12s), Epoch: [1/3], Step: [201/6668], Train Loss: 1.2886161804199219\n",
      "Time: 68m 27s (- -69m 52s), Epoch: [1/3], Step: [202/6668], Train Loss: 1.7965177917480468\n",
      "Time: 68m 47s (- -69m 33s), Epoch: [1/3], Step: [203/6668], Train Loss: 1.398542938232422\n",
      "Time: 69m 4s (- -69m 15s), Epoch: [1/3], Step: [204/6668], Train Loss: 1.8266131591796875\n",
      "Time: 69m 22s (- -70m 57s), Epoch: [1/3], Step: [205/6668], Train Loss: 1.7601998901367188\n",
      "Time: 69m 42s (- -70m 37s), Epoch: [1/3], Step: [206/6668], Train Loss: 2.109688262939453\n",
      "Time: 70m 2s (- -70m 18s), Epoch: [1/3], Step: [207/6668], Train Loss: 1.9351136779785156\n",
      "Time: 70m 21s (- -71m 58s), Epoch: [1/3], Step: [208/6668], Train Loss: 1.7536813354492187\n",
      "Time: 70m 41s (- -71m 39s), Epoch: [1/3], Step: [209/6668], Train Loss: 1.8108726501464845\n",
      "Time: 70m 58s (- -71m 21s), Epoch: [1/3], Step: [210/6668], Train Loss: 1.4623826599121095\n",
      "Time: 71m 16s (- -71m 4s), Epoch: [1/3], Step: [211/6668], Train Loss: 1.6488970947265624\n",
      "Time: 71m 35s (- -72m 44s), Epoch: [1/3], Step: [212/6668], Train Loss: 1.9669683837890626\n",
      "Time: 71m 53s (- -72m 27s), Epoch: [1/3], Step: [213/6668], Train Loss: 1.4570294189453126\n",
      "Time: 72m 12s (- -72m 7s), Epoch: [1/3], Step: [214/6668], Train Loss: 1.758175048828125\n",
      "Time: 72m 30s (- -73m 49s), Epoch: [1/3], Step: [215/6668], Train Loss: 1.4468161010742187\n",
      "Time: 72m 47s (- -73m 32s), Epoch: [1/3], Step: [216/6668], Train Loss: 1.9138583374023437\n",
      "Time: 73m 5s (- -73m 14s), Epoch: [1/3], Step: [217/6668], Train Loss: 1.7227540588378907\n",
      "Time: 73m 23s (- -74m 56s), Epoch: [1/3], Step: [218/6668], Train Loss: 1.4452371215820312\n",
      "Time: 73m 42s (- -74m 37s), Epoch: [1/3], Step: [219/6668], Train Loss: 1.5585343933105469\n",
      "Time: 74m 0s (- -74m 19s), Epoch: [1/3], Step: [220/6668], Train Loss: 1.3020625305175781\n",
      "Time: 74m 17s (- -74m 2s), Epoch: [1/3], Step: [221/6668], Train Loss: 1.8238107299804687\n",
      "Time: 74m 34s (- -75m 45s), Epoch: [1/3], Step: [222/6668], Train Loss: 2.0170936584472656\n",
      "Time: 74m 52s (- -75m 27s), Epoch: [1/3], Step: [223/6668], Train Loss: 1.9574040222167968\n",
      "Time: 75m 11s (- -75m 8s), Epoch: [1/3], Step: [224/6668], Train Loss: 1.9901171875\n",
      "Time: 75m 29s (- -76m 50s), Epoch: [1/3], Step: [225/6668], Train Loss: 1.4539204406738282\n",
      "Time: 75m 50s (- -76m 30s), Epoch: [1/3], Step: [226/6668], Train Loss: 2.1013885498046876\n",
      "Time: 76m 7s (- -76m 12s), Epoch: [1/3], Step: [227/6668], Train Loss: 1.319689483642578\n",
      "Time: 76m 24s (- -77m 55s), Epoch: [1/3], Step: [228/6668], Train Loss: 1.5619732666015624\n",
      "Time: 76m 42s (- -77m 37s), Epoch: [1/3], Step: [229/6668], Train Loss: 2.0483827209472656\n",
      "Time: 77m 1s (- -77m 18s), Epoch: [1/3], Step: [230/6668], Train Loss: 1.5517124938964844\n",
      "Time: 77m 21s (- -78m 58s), Epoch: [1/3], Step: [231/6668], Train Loss: 1.300938720703125\n",
      "Time: 77m 41s (- -78m 38s), Epoch: [1/3], Step: [232/6668], Train Loss: 1.40714111328125\n",
      "Time: 78m 1s (- -78m 18s), Epoch: [1/3], Step: [233/6668], Train Loss: 1.7156600952148438\n",
      "Time: 78m 18s (- -78m 1s), Epoch: [1/3], Step: [234/6668], Train Loss: 1.9022628784179687\n",
      "Time: 78m 36s (- -79m 43s), Epoch: [1/3], Step: [235/6668], Train Loss: 1.4943942260742187\n",
      "Time: 78m 53s (- -79m 26s), Epoch: [1/3], Step: [236/6668], Train Loss: 2.0120204162597655\n",
      "Time: 79m 13s (- -79m 6s), Epoch: [1/3], Step: [237/6668], Train Loss: 1.7643353271484374\n",
      "Time: 79m 30s (- -80m 49s), Epoch: [1/3], Step: [238/6668], Train Loss: 1.7053523254394531\n",
      "Time: 79m 50s (- -80m 29s), Epoch: [1/3], Step: [239/6668], Train Loss: 1.628206787109375\n",
      "Time: 80m 10s (- -80m 9s), Epoch: [1/3], Step: [240/6668], Train Loss: 1.5966011047363282\n",
      "Time: 80m 29s (- -81m 50s), Epoch: [1/3], Step: [241/6668], Train Loss: 1.5588381958007813\n",
      "Time: 80m 49s (- -81m 30s), Epoch: [1/3], Step: [242/6668], Train Loss: 1.950850067138672\n",
      "Time: 81m 7s (- -81m 12s), Epoch: [1/3], Step: [243/6668], Train Loss: 1.5575749206542968\n",
      "Time: 81m 28s (- -82m 51s), Epoch: [1/3], Step: [244/6668], Train Loss: 1.7531488037109375\n",
      "Time: 81m 48s (- -82m 31s), Epoch: [1/3], Step: [245/6668], Train Loss: 1.8930923461914062\n",
      "Time: 82m 7s (- -82m 12s), Epoch: [1/3], Step: [246/6668], Train Loss: 1.7152940368652343\n",
      "Time: 82m 25s (- -83m 54s), Epoch: [1/3], Step: [247/6668], Train Loss: 1.4388934326171876\n",
      "Time: 82m 44s (- -83m 35s), Epoch: [1/3], Step: [248/6668], Train Loss: 1.5721888732910156\n",
      "Time: 83m 2s (- -83m 17s), Epoch: [1/3], Step: [249/6668], Train Loss: 1.7519841003417969\n",
      "Time: 83m 22s (- -84m 57s), Epoch: [1/3], Step: [250/6668], Train Loss: 1.4871733093261719\n",
      "Time: 83m 41s (- -84m 38s), Epoch: [1/3], Step: [251/6668], Train Loss: 1.7702798461914062\n",
      "Time: 84m 1s (- -84m 18s), Epoch: [1/3], Step: [252/6668], Train Loss: 1.9831227111816405\n",
      "Time: 84m 21s (- -85m 58s), Epoch: [1/3], Step: [253/6668], Train Loss: 1.4917539978027343\n",
      "Time: 84m 38s (- -85m 41s), Epoch: [1/3], Step: [254/6668], Train Loss: 1.832830352783203\n",
      "Time: 84m 55s (- -85m 24s), Epoch: [1/3], Step: [255/6668], Train Loss: 1.3503306579589844\n",
      "Time: 85m 13s (- -85m 6s), Epoch: [1/3], Step: [256/6668], Train Loss: 1.9056068420410157\n",
      "Time: 85m 31s (- -86m 48s), Epoch: [1/3], Step: [257/6668], Train Loss: 2.1725770568847658\n",
      "Time: 85m 50s (- -86m 29s), Epoch: [1/3], Step: [258/6668], Train Loss: 1.7944053649902343\n",
      "Time: 86m 8s (- -86m 11s), Epoch: [1/3], Step: [259/6668], Train Loss: 1.6966358947753906\n",
      "Time: 86m 28s (- -87m 51s), Epoch: [1/3], Step: [260/6668], Train Loss: 1.7431410217285157\n",
      "Time: 86m 47s (- -87m 32s), Epoch: [1/3], Step: [261/6668], Train Loss: 1.5804493713378907\n",
      "Time: 87m 7s (- -87m 12s), Epoch: [1/3], Step: [262/6668], Train Loss: 2.055074005126953\n",
      "Time: 87m 25s (- -88m 54s), Epoch: [1/3], Step: [263/6668], Train Loss: 1.7875082397460937\n",
      "Time: 87m 42s (- -88m 37s), Epoch: [1/3], Step: [264/6668], Train Loss: 1.6460090637207032\n",
      "Time: 88m 1s (- -88m 18s), Epoch: [1/3], Step: [265/6668], Train Loss: 1.4515794372558595\n",
      "Time: 88m 23s (- -89m 56s), Epoch: [1/3], Step: [266/6668], Train Loss: 1.5684825134277345\n",
      "Time: 88m 40s (- -89m 39s), Epoch: [1/3], Step: [267/6668], Train Loss: 1.4123074340820312\n",
      "Time: 88m 57s (- -89m 21s), Epoch: [1/3], Step: [268/6668], Train Loss: 1.7151496887207032\n",
      "Time: 89m 17s (- -89m 2s), Epoch: [1/3], Step: [269/6668], Train Loss: 1.590731201171875\n",
      "Time: 89m 34s (- -90m 45s), Epoch: [1/3], Step: [270/6668], Train Loss: 1.5345046997070313\n",
      "Time: 89m 52s (- -90m 27s), Epoch: [1/3], Step: [271/6668], Train Loss: 1.7093118286132813\n",
      "Time: 90m 11s (- -90m 8s), Epoch: [1/3], Step: [272/6668], Train Loss: 1.5464814758300782\n",
      "Time: 90m 29s (- -91m 50s), Epoch: [1/3], Step: [273/6668], Train Loss: 1.6205978393554688\n",
      "Time: 90m 46s (- -91m 32s), Epoch: [1/3], Step: [274/6668], Train Loss: 1.7638900756835938\n",
      "Time: 91m 5s (- -91m 14s), Epoch: [1/3], Step: [275/6668], Train Loss: 1.652346649169922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 91m 25s (- -92m 54s), Epoch: [1/3], Step: [276/6668], Train Loss: 1.4959503173828126\n",
      "Time: 91m 43s (- -92m 36s), Epoch: [1/3], Step: [277/6668], Train Loss: 1.6247982788085937\n",
      "Time: 92m 2s (- -92m 16s), Epoch: [1/3], Step: [278/6668], Train Loss: 1.583105010986328\n",
      "Time: 92m 22s (- -93m 57s), Epoch: [1/3], Step: [279/6668], Train Loss: 1.7925144958496093\n",
      "Time: 92m 42s (- -93m 37s), Epoch: [1/3], Step: [280/6668], Train Loss: 1.5568218994140626\n",
      "Time: 92m 59s (- -93m 20s), Epoch: [1/3], Step: [281/6668], Train Loss: 1.52027099609375\n",
      "Time: 93m 17s (- -93m 2s), Epoch: [1/3], Step: [282/6668], Train Loss: 1.5665008544921875\n",
      "Time: 93m 34s (- -94m 45s), Epoch: [1/3], Step: [283/6668], Train Loss: 1.71555419921875\n",
      "Time: 93m 52s (- -94m 27s), Epoch: [1/3], Step: [284/6668], Train Loss: 1.830846405029297\n",
      "Time: 94m 12s (- -94m 7s), Epoch: [1/3], Step: [285/6668], Train Loss: 1.356453399658203\n",
      "Time: 94m 31s (- -95m 48s), Epoch: [1/3], Step: [286/6668], Train Loss: 1.8201727294921874\n",
      "Time: 94m 49s (- -95m 30s), Epoch: [1/3], Step: [287/6668], Train Loss: 1.8169558715820313\n",
      "Time: 95m 8s (- -95m 11s), Epoch: [1/3], Step: [288/6668], Train Loss: 1.5373143005371093\n",
      "Time: 95m 27s (- -96m 51s), Epoch: [1/3], Step: [289/6668], Train Loss: 1.8542626953125\n",
      "Time: 95m 45s (- -96m 34s), Epoch: [1/3], Step: [290/6668], Train Loss: 1.7206365966796875\n",
      "Time: 96m 3s (- -96m 16s), Epoch: [1/3], Step: [291/6668], Train Loss: 1.3710427856445313\n",
      "Time: 96m 22s (- -97m 57s), Epoch: [1/3], Step: [292/6668], Train Loss: 1.6448696899414061\n",
      "Time: 96m 40s (- -97m 39s), Epoch: [1/3], Step: [293/6668], Train Loss: 1.496053466796875\n",
      "Time: 96m 57s (- -97m 22s), Epoch: [1/3], Step: [294/6668], Train Loss: 1.7357440185546875\n",
      "Time: 97m 14s (- -97m 5s), Epoch: [1/3], Step: [295/6668], Train Loss: 1.42435546875\n",
      "Time: 97m 34s (- -98m 45s), Epoch: [1/3], Step: [296/6668], Train Loss: 1.794803924560547\n",
      "Time: 97m 53s (- -98m 26s), Epoch: [1/3], Step: [297/6668], Train Loss: 1.9475852966308593\n",
      "Time: 98m 13s (- -98m 6s), Epoch: [1/3], Step: [298/6668], Train Loss: 1.7593223571777343\n",
      "Time: 98m 33s (- -99m 46s), Epoch: [1/3], Step: [299/6668], Train Loss: 1.5240574645996094\n",
      "Time: 98m 53s (- -99m 26s), Epoch: [1/3], Step: [300/6668], Train Loss: 1.6648654174804687\n",
      "Time: 99m 10s (- -99m 9s), Epoch: [1/3], Step: [301/6668], Train Loss: 1.6894056701660156\n",
      "Time: 99m 28s (- -100m 51s), Epoch: [1/3], Step: [302/6668], Train Loss: 1.6425521850585938\n",
      "Time: 99m 45s (- -100m 33s), Epoch: [1/3], Step: [303/6668], Train Loss: 1.5410865783691405\n",
      "Time: 100m 3s (- -100m 16s), Epoch: [1/3], Step: [304/6668], Train Loss: 1.9856550598144531\n",
      "Time: 100m 22s (- -101m 56s), Epoch: [1/3], Step: [305/6668], Train Loss: 1.566846923828125\n",
      "Time: 100m 43s (- -101m 35s), Epoch: [1/3], Step: [306/6668], Train Loss: 1.9187348937988282\n",
      "Time: 101m 1s (- -101m 18s), Epoch: [1/3], Step: [307/6668], Train Loss: 2.3859101867675783\n",
      "Time: 101m 20s (- -102m 58s), Epoch: [1/3], Step: [308/6668], Train Loss: 1.7312617492675781\n",
      "Time: 101m 40s (- -102m 39s), Epoch: [1/3], Step: [309/6668], Train Loss: 1.3739242553710938\n",
      "Time: 101m 59s (- -102m 19s), Epoch: [1/3], Step: [310/6668], Train Loss: 2.13845458984375\n",
      "Time: 102m 17s (- -102m 2s), Epoch: [1/3], Step: [311/6668], Train Loss: 1.4972779846191406\n",
      "Time: 102m 34s (- -103m 44s), Epoch: [1/3], Step: [312/6668], Train Loss: 2.138041534423828\n",
      "Time: 102m 52s (- -103m 27s), Epoch: [1/3], Step: [313/6668], Train Loss: 1.6552622985839844\n",
      "Time: 103m 9s (- -103m 10s), Epoch: [1/3], Step: [314/6668], Train Loss: 1.6539009094238282\n",
      "Time: 103m 29s (- -104m 50s), Epoch: [1/3], Step: [315/6668], Train Loss: 1.9036050415039063\n",
      "Time: 103m 49s (- -104m 30s), Epoch: [1/3], Step: [316/6668], Train Loss: 1.7656562805175782\n",
      "Time: 104m 6s (- -104m 13s), Epoch: [1/3], Step: [317/6668], Train Loss: 1.751793212890625\n",
      "Time: 104m 25s (- -105m 53s), Epoch: [1/3], Step: [318/6668], Train Loss: 1.9176861572265624\n",
      "Time: 104m 43s (- -105m 36s), Epoch: [1/3], Step: [319/6668], Train Loss: 1.8835826110839844\n",
      "Time: 105m 2s (- -105m 17s), Epoch: [1/3], Step: [320/6668], Train Loss: 1.5608732604980469\n",
      "Time: 105m 21s (- -106m 57s), Epoch: [1/3], Step: [321/6668], Train Loss: 1.7766677856445312\n",
      "Time: 105m 39s (- -106m 39s), Epoch: [1/3], Step: [322/6668], Train Loss: 1.7558683776855468\n",
      "Time: 105m 57s (- -106m 22s), Epoch: [1/3], Step: [323/6668], Train Loss: 1.5669248962402345\n",
      "Time: 106m 14s (- -106m 5s), Epoch: [1/3], Step: [324/6668], Train Loss: 1.376244354248047\n",
      "Time: 106m 32s (- -107m 47s), Epoch: [1/3], Step: [325/6668], Train Loss: 1.648231201171875\n",
      "Time: 106m 49s (- -107m 29s), Epoch: [1/3], Step: [326/6668], Train Loss: 2.2416941833496096\n",
      "Time: 107m 9s (- -107m 10s), Epoch: [1/3], Step: [327/6668], Train Loss: 1.6142448425292968\n",
      "Time: 107m 29s (- -108m 50s), Epoch: [1/3], Step: [328/6668], Train Loss: 1.5173715209960938\n",
      "Time: 107m 48s (- -108m 31s), Epoch: [1/3], Step: [329/6668], Train Loss: 1.7819296264648437\n",
      "Time: 108m 8s (- -108m 11s), Epoch: [1/3], Step: [330/6668], Train Loss: 1.846025390625\n",
      "Time: 108m 25s (- -109m 53s), Epoch: [1/3], Step: [331/6668], Train Loss: 1.3997003173828124\n",
      "Time: 108m 45s (- -109m 34s), Epoch: [1/3], Step: [332/6668], Train Loss: 1.3421022033691405\n",
      "Time: 109m 5s (- -109m 14s), Epoch: [1/3], Step: [333/6668], Train Loss: 1.6247303771972657\n",
      "Time: 109m 22s (- -110m 57s), Epoch: [1/3], Step: [334/6668], Train Loss: 1.6233805847167968\n",
      "Time: 109m 41s (- -110m 37s), Epoch: [1/3], Step: [335/6668], Train Loss: 1.7119728088378907\n",
      "Time: 109m 59s (- -110m 20s), Epoch: [1/3], Step: [336/6668], Train Loss: 1.9303604125976563\n",
      "Time: 110m 16s (- -110m 2s), Epoch: [1/3], Step: [337/6668], Train Loss: 1.60040283203125\n",
      "Time: 110m 36s (- -111m 42s), Epoch: [1/3], Step: [338/6668], Train Loss: 1.5088528442382811\n",
      "Time: 110m 54s (- -111m 25s), Epoch: [1/3], Step: [339/6668], Train Loss: 1.2177188110351562\n",
      "Time: 111m 13s (- -111m 5s), Epoch: [1/3], Step: [340/6668], Train Loss: 2.098513946533203\n",
      "Time: 111m 33s (- -112m 45s), Epoch: [1/3], Step: [341/6668], Train Loss: 2.063004455566406\n",
      "Time: 111m 53s (- -112m 26s), Epoch: [1/3], Step: [342/6668], Train Loss: 2.1720622253417967\n",
      "Time: 112m 12s (- -112m 6s), Epoch: [1/3], Step: [343/6668], Train Loss: 2.1868026733398436\n",
      "Time: 112m 32s (- -113m 47s), Epoch: [1/3], Step: [344/6668], Train Loss: 1.4026071166992187\n",
      "Time: 112m 49s (- -113m 29s), Epoch: [1/3], Step: [345/6668], Train Loss: 1.8728317260742187\n",
      "Time: 113m 9s (- -113m 9s), Epoch: [1/3], Step: [346/6668], Train Loss: 1.4869561767578126\n",
      "Time: 113m 29s (- -114m 49s), Epoch: [1/3], Step: [347/6668], Train Loss: 1.625587921142578\n",
      "Time: 113m 49s (- -114m 30s), Epoch: [1/3], Step: [348/6668], Train Loss: 1.4330198669433594\n",
      "Time: 114m 8s (- -114m 11s), Epoch: [1/3], Step: [349/6668], Train Loss: 1.3599090576171875\n",
      "Time: 114m 27s (- -115m 52s), Epoch: [1/3], Step: [350/6668], Train Loss: 1.948407440185547\n",
      "Time: 114m 44s (- -115m 34s), Epoch: [1/3], Step: [351/6668], Train Loss: 1.5590072631835938\n",
      "Time: 115m 2s (- -115m 17s), Epoch: [1/3], Step: [352/6668], Train Loss: 1.8623533630371094\n",
      "Time: 115m 19s (- -116m 59s), Epoch: [1/3], Step: [353/6668], Train Loss: 1.717059326171875\n",
      "Time: 115m 41s (- -116m 38s), Epoch: [1/3], Step: [354/6668], Train Loss: 1.6038192749023437\n",
      "Time: 115m 58s (- -116m 20s), Epoch: [1/3], Step: [355/6668], Train Loss: 1.4458529663085937\n",
      "Time: 116m 16s (- -116m 3s), Epoch: [1/3], Step: [356/6668], Train Loss: 1.6690321350097657\n",
      "Time: 116m 35s (- -117m 43s), Epoch: [1/3], Step: [357/6668], Train Loss: 1.6382231140136718\n",
      "Time: 116m 53s (- -117m 26s), Epoch: [1/3], Step: [358/6668], Train Loss: 1.487261962890625\n",
      "Time: 117m 10s (- -117m 8s), Epoch: [1/3], Step: [359/6668], Train Loss: 1.769383544921875\n",
      "Time: 117m 30s (- -118m 49s), Epoch: [1/3], Step: [360/6668], Train Loss: 1.9307255554199219\n",
      "Time: 117m 47s (- -118m 31s), Epoch: [1/3], Step: [361/6668], Train Loss: 1.5858322143554688\n",
      "Time: 118m 5s (- -118m 14s), Epoch: [1/3], Step: [362/6668], Train Loss: 1.6725474548339845\n",
      "Time: 118m 26s (- -119m 52s), Epoch: [1/3], Step: [363/6668], Train Loss: 1.7273963928222655\n",
      "Time: 118m 44s (- -119m 35s), Epoch: [1/3], Step: [364/6668], Train Loss: 1.4779391479492188\n",
      "Time: 119m 4s (- -119m 15s), Epoch: [1/3], Step: [365/6668], Train Loss: 1.81566650390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 119m 23s (- -120m 55s), Epoch: [1/3], Step: [366/6668], Train Loss: 1.967689208984375\n",
      "Time: 119m 43s (- -120m 36s), Epoch: [1/3], Step: [367/6668], Train Loss: 1.448539276123047\n",
      "Time: 120m 2s (- -120m 16s), Epoch: [1/3], Step: [368/6668], Train Loss: 1.8818609619140625\n",
      "Time: 120m 20s (- -121m 59s), Epoch: [1/3], Step: [369/6668], Train Loss: 1.4718490600585938\n",
      "Time: 120m 37s (- -121m 41s), Epoch: [1/3], Step: [370/6668], Train Loss: 1.5973048400878906\n",
      "Time: 120m 58s (- -121m 21s), Epoch: [1/3], Step: [371/6668], Train Loss: 1.4308395385742188\n",
      "Time: 121m 15s (- -121m 3s), Epoch: [1/3], Step: [372/6668], Train Loss: 1.6964529418945313\n",
      "Time: 121m 33s (- -122m 45s), Epoch: [1/3], Step: [373/6668], Train Loss: 1.7737437438964845\n",
      "Time: 121m 53s (- -122m 26s), Epoch: [1/3], Step: [374/6668], Train Loss: 1.6908680725097656\n",
      "Time: 122m 11s (- -122m 8s), Epoch: [1/3], Step: [375/6668], Train Loss: 1.8548619079589843\n",
      "Time: 122m 30s (- -123m 48s), Epoch: [1/3], Step: [376/6668], Train Loss: 1.6570524597167968\n",
      "Time: 122m 48s (- -123m 31s), Epoch: [1/3], Step: [377/6668], Train Loss: 1.992680206298828\n",
      "Time: 123m 5s (- -123m 13s), Epoch: [1/3], Step: [378/6668], Train Loss: 1.635725860595703\n",
      "Time: 123m 23s (- -124m 56s), Epoch: [1/3], Step: [379/6668], Train Loss: 1.6978884887695314\n",
      "Time: 123m 41s (- -124m 38s), Epoch: [1/3], Step: [380/6668], Train Loss: 1.8617889404296875\n",
      "Time: 124m 0s (- -124m 18s), Epoch: [1/3], Step: [381/6668], Train Loss: 2.143743591308594\n",
      "Time: 124m 20s (- -125m 59s), Epoch: [1/3], Step: [382/6668], Train Loss: 1.4860111999511718\n",
      "Time: 124m 39s (- -125m 39s), Epoch: [1/3], Step: [383/6668], Train Loss: 1.5666783142089844\n",
      "Time: 124m 59s (- -125m 19s), Epoch: [1/3], Step: [384/6668], Train Loss: 1.8013592529296876\n",
      "Time: 125m 17s (- -125m 2s), Epoch: [1/3], Step: [385/6668], Train Loss: 1.8975709533691407\n",
      "Time: 125m 36s (- -126m 42s), Epoch: [1/3], Step: [386/6668], Train Loss: 1.742091522216797\n",
      "Time: 125m 56s (- -126m 22s), Epoch: [1/3], Step: [387/6668], Train Loss: 1.4870100402832032\n",
      "Time: 126m 16s (- -126m 3s), Epoch: [1/3], Step: [388/6668], Train Loss: 2.01282958984375\n",
      "Time: 126m 35s (- -127m 43s), Epoch: [1/3], Step: [389/6668], Train Loss: 1.5783648681640625\n",
      "Time: 126m 55s (- -127m 24s), Epoch: [1/3], Step: [390/6668], Train Loss: 1.677931365966797\n",
      "Time: 127m 14s (- -127m 4s), Epoch: [1/3], Step: [391/6668], Train Loss: 1.6705413818359376\n",
      "Time: 127m 33s (- -128m 45s), Epoch: [1/3], Step: [392/6668], Train Loss: 1.8408943176269532\n",
      "Time: 127m 51s (- -128m 28s), Epoch: [1/3], Step: [393/6668], Train Loss: 1.5750807189941407\n",
      "Time: 128m 10s (- -128m 8s), Epoch: [1/3], Step: [394/6668], Train Loss: 1.9068318176269532\n",
      "Time: 128m 30s (- -129m 49s), Epoch: [1/3], Step: [395/6668], Train Loss: 1.6553573608398438\n",
      "Time: 128m 49s (- -129m 29s), Epoch: [1/3], Step: [396/6668], Train Loss: 1.8758395385742188\n",
      "Time: 129m 9s (- -129m 10s), Epoch: [1/3], Step: [397/6668], Train Loss: 1.4821595764160156\n",
      "Time: 129m 29s (- -130m 50s), Epoch: [1/3], Step: [398/6668], Train Loss: 1.9010641479492187\n",
      "Time: 129m 48s (- -130m 30s), Epoch: [1/3], Step: [399/6668], Train Loss: 1.6566395568847656\n",
      "Time: 130m 6s (- -130m 13s), Epoch: [1/3], Step: [400/6668], Train Loss: 1.8859805297851562\n",
      "Time: 130m 26s (- -131m 53s), Epoch: [1/3], Step: [401/6668], Train Loss: 1.356105499267578\n",
      "Time: 130m 43s (- -131m 35s), Epoch: [1/3], Step: [402/6668], Train Loss: 1.5754519653320314\n",
      "Time: 131m 3s (- -131m 16s), Epoch: [1/3], Step: [403/6668], Train Loss: 1.6924020385742187\n",
      "Time: 131m 23s (- -132m 56s), Epoch: [1/3], Step: [404/6668], Train Loss: 2.3236851501464844\n",
      "Time: 131m 40s (- -132m 39s), Epoch: [1/3], Step: [405/6668], Train Loss: 2.120835876464844\n",
      "Time: 131m 59s (- -132m 19s), Epoch: [1/3], Step: [406/6668], Train Loss: 1.4880616760253906\n",
      "Time: 132m 19s (- -132m 0s), Epoch: [1/3], Step: [407/6668], Train Loss: 1.664780731201172\n",
      "Time: 132m 36s (- -133m 42s), Epoch: [1/3], Step: [408/6668], Train Loss: 1.7831535339355469\n",
      "Time: 132m 56s (- -133m 23s), Epoch: [1/3], Step: [409/6668], Train Loss: 1.6111776733398437\n",
      "Time: 133m 17s (- -133m 2s), Epoch: [1/3], Step: [410/6668], Train Loss: 1.7811309814453125\n",
      "Time: 133m 35s (- -134m 44s), Epoch: [1/3], Step: [411/6668], Train Loss: 2.117603302001953\n",
      "Time: 133m 52s (- -134m 26s), Epoch: [1/3], Step: [412/6668], Train Loss: 1.6357447814941406\n",
      "Time: 134m 10s (- -134m 9s), Epoch: [1/3], Step: [413/6668], Train Loss: 1.9228012084960937\n",
      "Time: 134m 29s (- -135m 49s), Epoch: [1/3], Step: [414/6668], Train Loss: 1.75434326171875\n",
      "Time: 134m 49s (- -135m 30s), Epoch: [1/3], Step: [415/6668], Train Loss: 1.913812255859375\n",
      "Time: 135m 8s (- -135m 10s), Epoch: [1/3], Step: [416/6668], Train Loss: 1.470084228515625\n",
      "Time: 135m 26s (- -136m 53s), Epoch: [1/3], Step: [417/6668], Train Loss: 1.4164981079101562\n",
      "Time: 135m 43s (- -136m 35s), Epoch: [1/3], Step: [418/6668], Train Loss: 1.3755360412597657\n",
      "Time: 136m 1s (- -136m 18s), Epoch: [1/3], Step: [419/6668], Train Loss: 1.6516622924804687\n",
      "Time: 136m 20s (- -137m 58s), Epoch: [1/3], Step: [420/6668], Train Loss: 1.6838618469238282\n",
      "Time: 136m 38s (- -137m 41s), Epoch: [1/3], Step: [421/6668], Train Loss: 2.204175567626953\n",
      "Time: 136m 55s (- -137m 23s), Epoch: [1/3], Step: [422/6668], Train Loss: 1.777431640625\n",
      "Time: 137m 15s (- -137m 4s), Epoch: [1/3], Step: [423/6668], Train Loss: 2.044616394042969\n",
      "Time: 137m 33s (- -138m 46s), Epoch: [1/3], Step: [424/6668], Train Loss: 1.855434112548828\n",
      "Time: 137m 50s (- -138m 28s), Epoch: [1/3], Step: [425/6668], Train Loss: 1.6133566284179688\n",
      "Time: 138m 9s (- -138m 9s), Epoch: [1/3], Step: [426/6668], Train Loss: 1.5265248107910157\n",
      "Time: 138m 29s (- -139m 49s), Epoch: [1/3], Step: [427/6668], Train Loss: 2.2065966796875\n",
      "Time: 138m 49s (- -139m 30s), Epoch: [1/3], Step: [428/6668], Train Loss: 2.029696197509766\n",
      "Time: 139m 8s (- -139m 10s), Epoch: [1/3], Step: [429/6668], Train Loss: 2.211233367919922\n",
      "Time: 139m 28s (- -140m 50s), Epoch: [1/3], Step: [430/6668], Train Loss: 1.7382949829101562\n",
      "Time: 139m 46s (- -140m 33s), Epoch: [1/3], Step: [431/6668], Train Loss: 1.7351893615722656\n",
      "Time: 140m 3s (- -140m 15s), Epoch: [1/3], Step: [432/6668], Train Loss: 2.0188592529296874\n",
      "Time: 140m 23s (- -141m 56s), Epoch: [1/3], Step: [433/6668], Train Loss: 1.6629791259765625\n",
      "Time: 140m 41s (- -141m 38s), Epoch: [1/3], Step: [434/6668], Train Loss: 1.7306143188476562\n",
      "Time: 141m 0s (- -141m 18s), Epoch: [1/3], Step: [435/6668], Train Loss: 1.9892378234863282\n",
      "Time: 141m 20s (- -142m 58s), Epoch: [1/3], Step: [436/6668], Train Loss: 1.9884255981445313\n",
      "Time: 141m 38s (- -142m 41s), Epoch: [1/3], Step: [437/6668], Train Loss: 1.90818603515625\n",
      "Time: 141m 57s (- -142m 21s), Epoch: [1/3], Step: [438/6668], Train Loss: 1.9418768310546874\n",
      "Time: 142m 17s (- -142m 1s), Epoch: [1/3], Step: [439/6668], Train Loss: 1.5971672058105468\n",
      "Time: 142m 37s (- -143m 42s), Epoch: [1/3], Step: [440/6668], Train Loss: 1.7307992553710938\n",
      "Time: 142m 54s (- -143m 24s), Epoch: [1/3], Step: [441/6668], Train Loss: 1.4833448791503907\n",
      "Time: 143m 15s (- -143m 4s), Epoch: [1/3], Step: [442/6668], Train Loss: 1.7626092529296875\n",
      "Time: 143m 35s (- -144m 43s), Epoch: [1/3], Step: [443/6668], Train Loss: 1.7736863708496093\n",
      "Time: 143m 53s (- -144m 26s), Epoch: [1/3], Step: [444/6668], Train Loss: 2.085107879638672\n",
      "Time: 144m 10s (- -144m 8s), Epoch: [1/3], Step: [445/6668], Train Loss: 1.5585765075683593\n",
      "Time: 144m 30s (- -145m 49s), Epoch: [1/3], Step: [446/6668], Train Loss: 1.92939697265625\n",
      "Time: 144m 50s (- -145m 29s), Epoch: [1/3], Step: [447/6668], Train Loss: 1.722733917236328\n",
      "Time: 145m 7s (- -145m 11s), Epoch: [1/3], Step: [448/6668], Train Loss: 1.6448257446289063\n",
      "Time: 145m 27s (- -146m 52s), Epoch: [1/3], Step: [449/6668], Train Loss: 1.8189337158203125\n",
      "Time: 145m 44s (- -146m 34s), Epoch: [1/3], Step: [450/6668], Train Loss: 1.4707403564453125\n",
      "Time: 146m 2s (- -146m 17s), Epoch: [1/3], Step: [451/6668], Train Loss: 1.6112960815429687\n",
      "Time: 146m 22s (- -147m 57s), Epoch: [1/3], Step: [452/6668], Train Loss: 1.9903285217285156\n",
      "Time: 146m 41s (- -147m 38s), Epoch: [1/3], Step: [453/6668], Train Loss: 2.0467265319824217\n",
      "Time: 146m 58s (- -147m 20s), Epoch: [1/3], Step: [454/6668], Train Loss: 1.7724490356445313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 147m 16s (- -147m 3s), Epoch: [1/3], Step: [455/6668], Train Loss: 1.843590087890625\n",
      "Time: 147m 33s (- -148m 45s), Epoch: [1/3], Step: [456/6668], Train Loss: 1.7165211486816405\n",
      "Time: 147m 51s (- -148m 28s), Epoch: [1/3], Step: [457/6668], Train Loss: 1.9577841186523437\n",
      "Time: 148m 8s (- -148m 10s), Epoch: [1/3], Step: [458/6668], Train Loss: 1.841990966796875\n",
      "Time: 148m 29s (- -149m 49s), Epoch: [1/3], Step: [459/6668], Train Loss: 1.561771240234375\n",
      "Time: 148m 47s (- -149m 31s), Epoch: [1/3], Step: [460/6668], Train Loss: 1.7286647033691407\n",
      "Time: 149m 7s (- -149m 12s), Epoch: [1/3], Step: [461/6668], Train Loss: 1.5310670471191405\n",
      "Time: 149m 26s (- -150m 52s), Epoch: [1/3], Step: [462/6668], Train Loss: 1.5324465942382812\n",
      "Time: 149m 44s (- -150m 34s), Epoch: [1/3], Step: [463/6668], Train Loss: 1.919459686279297\n",
      "Time: 150m 4s (- -150m 15s), Epoch: [1/3], Step: [464/6668], Train Loss: 1.8672561645507812\n",
      "Time: 150m 23s (- -151m 55s), Epoch: [1/3], Step: [465/6668], Train Loss: 2.1168582153320314\n",
      "Time: 150m 41s (- -151m 37s), Epoch: [1/3], Step: [466/6668], Train Loss: 1.9008975219726563\n",
      "Time: 150m 59s (- -151m 20s), Epoch: [1/3], Step: [467/6668], Train Loss: 1.952283935546875\n",
      "Time: 151m 18s (- -151m 0s), Epoch: [1/3], Step: [468/6668], Train Loss: 1.5605158996582031\n",
      "Time: 151m 38s (- -152m 40s), Epoch: [1/3], Step: [469/6668], Train Loss: 1.6197920227050782\n",
      "Time: 151m 57s (- -152m 21s), Epoch: [1/3], Step: [470/6668], Train Loss: 1.955428924560547\n",
      "Time: 152m 15s (- -152m 4s), Epoch: [1/3], Step: [471/6668], Train Loss: 1.3425234985351562\n",
      "Time: 152m 35s (- -153m 44s), Epoch: [1/3], Step: [472/6668], Train Loss: 1.7786294555664062\n",
      "Time: 152m 52s (- -153m 26s), Epoch: [1/3], Step: [473/6668], Train Loss: 1.6044125366210937\n",
      "Time: 153m 10s (- -153m 9s), Epoch: [1/3], Step: [474/6668], Train Loss: 1.757288055419922\n",
      "Time: 153m 30s (- -154m 49s), Epoch: [1/3], Step: [475/6668], Train Loss: 1.6424055480957032\n",
      "Time: 153m 49s (- -154m 29s), Epoch: [1/3], Step: [476/6668], Train Loss: 1.663636474609375\n",
      "Time: 154m 7s (- -154m 12s), Epoch: [1/3], Step: [477/6668], Train Loss: 1.7092776489257813\n",
      "Time: 154m 24s (- -155m 54s), Epoch: [1/3], Step: [478/6668], Train Loss: 1.8504397583007812\n",
      "Time: 154m 42s (- -155m 37s), Epoch: [1/3], Step: [479/6668], Train Loss: 1.8862841796875\n",
      "Time: 154m 59s (- -155m 19s), Epoch: [1/3], Step: [480/6668], Train Loss: 1.674656524658203\n",
      "Time: 155m 19s (- -155m 0s), Epoch: [1/3], Step: [481/6668], Train Loss: 1.6464750671386719\n",
      "Time: 155m 37s (- -156m 42s), Epoch: [1/3], Step: [482/6668], Train Loss: 1.6442210388183593\n",
      "Time: 155m 54s (- -156m 24s), Epoch: [1/3], Step: [483/6668], Train Loss: 1.9891374206542969\n",
      "Time: 156m 12s (- -156m 7s), Epoch: [1/3], Step: [484/6668], Train Loss: 1.7500650024414062\n",
      "Time: 156m 29s (- -157m 49s), Epoch: [1/3], Step: [485/6668], Train Loss: 1.6995951843261718\n",
      "Time: 156m 49s (- -157m 30s), Epoch: [1/3], Step: [486/6668], Train Loss: 1.886602783203125\n",
      "Time: 157m 8s (- -157m 10s), Epoch: [1/3], Step: [487/6668], Train Loss: 1.3613316345214843\n",
      "Time: 157m 26s (- -158m 52s), Epoch: [1/3], Step: [488/6668], Train Loss: 1.8284390258789063\n",
      "Time: 157m 47s (- -158m 31s), Epoch: [1/3], Step: [489/6668], Train Loss: 1.6530079650878906\n",
      "Time: 158m 7s (- -158m 11s), Epoch: [1/3], Step: [490/6668], Train Loss: 1.8086151123046874\n",
      "Time: 158m 25s (- -159m 53s), Epoch: [1/3], Step: [491/6668], Train Loss: 1.6582046508789063\n",
      "Time: 158m 45s (- -159m 34s), Epoch: [1/3], Step: [492/6668], Train Loss: 1.7184042358398437\n",
      "Time: 159m 4s (- -159m 14s), Epoch: [1/3], Step: [493/6668], Train Loss: 1.9053060913085937\n",
      "Time: 159m 22s (- -160m 57s), Epoch: [1/3], Step: [494/6668], Train Loss: 1.9969224548339843\n",
      "Time: 159m 39s (- -160m 39s), Epoch: [1/3], Step: [495/6668], Train Loss: 2.1804377746582033\n",
      "Time: 159m 57s (- -160m 22s), Epoch: [1/3], Step: [496/6668], Train Loss: 1.6722645568847656\n",
      "Time: 160m 14s (- -160m 4s), Epoch: [1/3], Step: [497/6668], Train Loss: 1.8579957580566406\n",
      "Time: 160m 34s (- -161m 45s), Epoch: [1/3], Step: [498/6668], Train Loss: 1.3831259155273437\n",
      "Time: 160m 51s (- -161m 27s), Epoch: [1/3], Step: [499/6668], Train Loss: 2.0016139221191405\n",
      "Time: 161m 10s (- -161m 8s), Epoch: [1/3], Step: [500/6668], Train Loss: 1.9491929626464843\n",
      "Time: 161m 30s (- -162m 48s), Epoch: [1/3], Step: [501/6668], Train Loss: 1.3479966735839843\n",
      "Time: 161m 48s (- -162m 31s), Epoch: [1/3], Step: [502/6668], Train Loss: 2.1131396484375\n",
      "Time: 162m 5s (- -162m 13s), Epoch: [1/3], Step: [503/6668], Train Loss: 1.7308433532714844\n",
      "Time: 162m 22s (- -163m 56s), Epoch: [1/3], Step: [504/6668], Train Loss: 2.231992034912109\n",
      "Time: 162m 40s (- -163m 39s), Epoch: [1/3], Step: [505/6668], Train Loss: 1.9542117309570313\n",
      "Time: 162m 57s (- -163m 21s), Epoch: [1/3], Step: [506/6668], Train Loss: 1.7052659606933593\n",
      "Time: 163m 14s (- -163m 4s), Epoch: [1/3], Step: [507/6668], Train Loss: 1.6875102233886718\n",
      "Time: 163m 32s (- -164m 46s), Epoch: [1/3], Step: [508/6668], Train Loss: 1.843743133544922\n",
      "Time: 163m 49s (- -164m 29s), Epoch: [1/3], Step: [509/6668], Train Loss: 1.6629658508300782\n",
      "Time: 164m 7s (- -164m 12s), Epoch: [1/3], Step: [510/6668], Train Loss: 1.7400888061523438\n",
      "Time: 164m 24s (- -165m 54s), Epoch: [1/3], Step: [511/6668], Train Loss: 1.9132833862304688\n",
      "Time: 164m 41s (- -165m 37s), Epoch: [1/3], Step: [512/6668], Train Loss: 1.4636154174804688\n",
      "Time: 164m 59s (- -165m 20s), Epoch: [1/3], Step: [513/6668], Train Loss: 1.7099301147460937\n",
      "Time: 165m 18s (- -165m 0s), Epoch: [1/3], Step: [514/6668], Train Loss: 1.6391946411132812\n",
      "Time: 165m 36s (- -166m 42s), Epoch: [1/3], Step: [515/6668], Train Loss: 2.059476013183594\n",
      "Time: 165m 56s (- -166m 23s), Epoch: [1/3], Step: [516/6668], Train Loss: 1.7846438598632812\n",
      "Time: 166m 15s (- -166m 4s), Epoch: [1/3], Step: [517/6668], Train Loss: 1.4815380859375\n",
      "Time: 166m 32s (- -167m 46s), Epoch: [1/3], Step: [518/6668], Train Loss: 2.1590733337402344\n",
      "Time: 166m 49s (- -167m 29s), Epoch: [1/3], Step: [519/6668], Train Loss: 1.8906770324707032\n",
      "Time: 167m 9s (- -167m 9s), Epoch: [1/3], Step: [520/6668], Train Loss: 1.7841859436035157\n",
      "Time: 167m 29s (- -168m 49s), Epoch: [1/3], Step: [521/6668], Train Loss: 1.7938592529296875\n",
      "Time: 167m 47s (- -168m 32s), Epoch: [1/3], Step: [522/6668], Train Loss: 1.9026618957519532\n",
      "Time: 168m 4s (- -168m 14s), Epoch: [1/3], Step: [523/6668], Train Loss: 1.5482078552246095\n",
      "Time: 168m 22s (- -169m 57s), Epoch: [1/3], Step: [524/6668], Train Loss: 1.916026611328125\n",
      "Time: 168m 39s (- -169m 39s), Epoch: [1/3], Step: [525/6668], Train Loss: 1.6237191772460937\n",
      "Time: 168m 59s (- -169m 19s), Epoch: [1/3], Step: [526/6668], Train Loss: 1.5718885803222655\n",
      "Time: 169m 18s (- -169m 0s), Epoch: [1/3], Step: [527/6668], Train Loss: 1.7220375061035156\n",
      "Time: 169m 36s (- -170m 42s), Epoch: [1/3], Step: [528/6668], Train Loss: 1.8177372741699218\n",
      "Time: 169m 53s (- -170m 25s), Epoch: [1/3], Step: [529/6668], Train Loss: 1.606844940185547\n",
      "Time: 170m 11s (- -170m 7s), Epoch: [1/3], Step: [530/6668], Train Loss: 2.2441952514648436\n",
      "Time: 170m 31s (- -171m 48s), Epoch: [1/3], Step: [531/6668], Train Loss: 1.801602783203125\n",
      "Time: 170m 51s (- -171m 28s), Epoch: [1/3], Step: [532/6668], Train Loss: 2.0322125244140623\n",
      "Time: 171m 8s (- -171m 10s), Epoch: [1/3], Step: [533/6668], Train Loss: 1.4835427856445313\n",
      "Time: 171m 28s (- -172m 50s), Epoch: [1/3], Step: [534/6668], Train Loss: 2.1409153747558594\n",
      "Time: 171m 46s (- -172m 33s), Epoch: [1/3], Step: [535/6668], Train Loss: 1.561431884765625\n",
      "Time: 172m 3s (- -172m 15s), Epoch: [1/3], Step: [536/6668], Train Loss: 1.6181437683105468\n",
      "Time: 172m 23s (- -173m 55s), Epoch: [1/3], Step: [537/6668], Train Loss: 2.0709854125976563\n",
      "Time: 172m 40s (- -173m 38s), Epoch: [1/3], Step: [538/6668], Train Loss: 2.3288230895996094\n",
      "Time: 173m 0s (- -173m 18s), Epoch: [1/3], Step: [539/6668], Train Loss: 1.5422134399414062\n",
      "Time: 173m 20s (- -174m 58s), Epoch: [1/3], Step: [540/6668], Train Loss: 1.402750244140625\n",
      "Time: 173m 40s (- -174m 39s), Epoch: [1/3], Step: [541/6668], Train Loss: 1.7735400390625\n",
      "Time: 173m 59s (- -174m 19s), Epoch: [1/3], Step: [542/6668], Train Loss: 1.9125311279296875\n",
      "Time: 174m 17s (- -174m 2s), Epoch: [1/3], Step: [543/6668], Train Loss: 1.8100332641601562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 174m 34s (- -175m 44s), Epoch: [1/3], Step: [544/6668], Train Loss: 1.8983291625976562\n",
      "Time: 174m 54s (- -175m 24s), Epoch: [1/3], Step: [545/6668], Train Loss: 1.8819035339355468\n",
      "Time: 175m 11s (- -175m 7s), Epoch: [1/3], Step: [546/6668], Train Loss: 1.7729766845703125\n",
      "Time: 175m 29s (- -176m 49s), Epoch: [1/3], Step: [547/6668], Train Loss: 1.9108641052246094\n",
      "Time: 175m 47s (- -176m 31s), Epoch: [1/3], Step: [548/6668], Train Loss: 1.8097640991210937\n",
      "Time: 176m 7s (- -176m 11s), Epoch: [1/3], Step: [549/6668], Train Loss: 1.7979391479492188\n",
      "Time: 176m 27s (- -177m 51s), Epoch: [1/3], Step: [550/6668], Train Loss: 2.0388223266601564\n",
      "Time: 176m 44s (- -177m 34s), Epoch: [1/3], Step: [551/6668], Train Loss: 1.7554461669921875\n",
      "Time: 177m 2s (- -177m 17s), Epoch: [1/3], Step: [552/6668], Train Loss: 1.71939208984375\n",
      "Time: 177m 19s (- -178m 59s), Epoch: [1/3], Step: [553/6668], Train Loss: 1.7433401489257812\n",
      "Time: 177m 39s (- -178m 40s), Epoch: [1/3], Step: [554/6668], Train Loss: 1.783209991455078\n",
      "Time: 177m 56s (- -178m 22s), Epoch: [1/3], Step: [555/6668], Train Loss: 1.4369525146484374\n",
      "Time: 178m 14s (- -178m 5s), Epoch: [1/3], Step: [556/6668], Train Loss: 1.4423947143554687\n",
      "Time: 178m 31s (- -179m 47s), Epoch: [1/3], Step: [557/6668], Train Loss: 1.7775372314453124\n",
      "Time: 178m 51s (- -179m 27s), Epoch: [1/3], Step: [558/6668], Train Loss: 1.3853233337402344\n",
      "Time: 179m 8s (- -179m 10s), Epoch: [1/3], Step: [559/6668], Train Loss: 1.3526698303222657\n",
      "Time: 179m 28s (- -180m 50s), Epoch: [1/3], Step: [560/6668], Train Loss: 1.93001220703125\n",
      "Time: 179m 48s (- -180m 31s), Epoch: [1/3], Step: [561/6668], Train Loss: 1.79308837890625\n",
      "Time: 180m 7s (- -180m 11s), Epoch: [1/3], Step: [562/6668], Train Loss: 1.7444894409179688\n",
      "Time: 180m 27s (- -181m 51s), Epoch: [1/3], Step: [563/6668], Train Loss: 1.4058676147460938\n",
      "Time: 180m 47s (- -181m 31s), Epoch: [1/3], Step: [564/6668], Train Loss: 1.5122055053710937\n",
      "Time: 181m 4s (- -181m 14s), Epoch: [1/3], Step: [565/6668], Train Loss: 2.1650843811035156\n",
      "Time: 181m 22s (- -182m 56s), Epoch: [1/3], Step: [566/6668], Train Loss: 1.7750114440917968\n",
      "Time: 181m 41s (- -182m 37s), Epoch: [1/3], Step: [567/6668], Train Loss: 2.2097984313964845\n",
      "Time: 181m 59s (- -182m 19s), Epoch: [1/3], Step: [568/6668], Train Loss: 1.84548095703125\n",
      "Time: 182m 19s (- -182m 0s), Epoch: [1/3], Step: [569/6668], Train Loss: 1.92209716796875\n",
      "Time: 182m 38s (- -183m 41s), Epoch: [1/3], Step: [570/6668], Train Loss: 1.658655242919922\n",
      "Time: 182m 57s (- -183m 21s), Epoch: [1/3], Step: [571/6668], Train Loss: 1.7323330688476561\n",
      "Time: 183m 17s (- -183m 1s), Epoch: [1/3], Step: [572/6668], Train Loss: 2.01204833984375\n",
      "Time: 183m 37s (- -184m 42s), Epoch: [1/3], Step: [573/6668], Train Loss: 1.6814767456054687\n",
      "Time: 183m 56s (- -184m 22s), Epoch: [1/3], Step: [574/6668], Train Loss: 1.489056854248047\n",
      "Time: 184m 16s (- -184m 3s), Epoch: [1/3], Step: [575/6668], Train Loss: 2.0261544799804687\n",
      "Time: 184m 33s (- -185m 45s), Epoch: [1/3], Step: [576/6668], Train Loss: 1.8939244079589843\n",
      "Time: 184m 51s (- -185m 28s), Epoch: [1/3], Step: [577/6668], Train Loss: 2.1229083251953127\n",
      "Time: 185m 8s (- -185m 10s), Epoch: [1/3], Step: [578/6668], Train Loss: 1.449263153076172\n",
      "Time: 185m 28s (- -186m 50s), Epoch: [1/3], Step: [579/6668], Train Loss: 1.8150794982910157\n",
      "Time: 185m 49s (- -186m 29s), Epoch: [1/3], Step: [580/6668], Train Loss: 2.02971923828125\n",
      "Time: 186m 7s (- -186m 11s), Epoch: [1/3], Step: [581/6668], Train Loss: 1.7519515991210937\n",
      "Time: 186m 27s (- -187m 51s), Epoch: [1/3], Step: [582/6668], Train Loss: 1.6170549011230468\n",
      "Time: 186m 46s (- -187m 32s), Epoch: [1/3], Step: [583/6668], Train Loss: 1.298456573486328\n",
      "Time: 187m 4s (- -187m 15s), Epoch: [1/3], Step: [584/6668], Train Loss: 2.0364776611328126\n",
      "Time: 187m 24s (- -188m 55s), Epoch: [1/3], Step: [585/6668], Train Loss: 2.1280172729492186\n",
      "Time: 187m 43s (- -188m 35s), Epoch: [1/3], Step: [586/6668], Train Loss: 2.14853515625\n",
      "Time: 188m 1s (- -188m 17s), Epoch: [1/3], Step: [587/6668], Train Loss: 1.4989163208007812\n",
      "Time: 188m 18s (- -188m 0s), Epoch: [1/3], Step: [588/6668], Train Loss: 1.7517433166503906\n",
      "Time: 188m 38s (- -189m 41s), Epoch: [1/3], Step: [589/6668], Train Loss: 1.9084764099121094\n",
      "Time: 188m 57s (- -189m 22s), Epoch: [1/3], Step: [590/6668], Train Loss: 2.1502255249023436\n",
      "Time: 189m 14s (- -189m 4s), Epoch: [1/3], Step: [591/6668], Train Loss: 1.859810791015625\n",
      "Time: 189m 32s (- -190m 47s), Epoch: [1/3], Step: [592/6668], Train Loss: 1.6126805114746094\n",
      "Time: 189m 49s (- -190m 29s), Epoch: [1/3], Step: [593/6668], Train Loss: 1.811061248779297\n",
      "Time: 190m 7s (- -190m 12s), Epoch: [1/3], Step: [594/6668], Train Loss: 1.4959182739257812\n",
      "Time: 190m 26s (- -191m 52s), Epoch: [1/3], Step: [595/6668], Train Loss: 1.5568922424316407\n",
      "Time: 190m 46s (- -191m 32s), Epoch: [1/3], Step: [596/6668], Train Loss: 2.0445516967773436\n",
      "Time: 191m 3s (- -191m 15s), Epoch: [1/3], Step: [597/6668], Train Loss: 1.7452052307128907\n",
      "Time: 191m 23s (- -192m 55s), Epoch: [1/3], Step: [598/6668], Train Loss: 1.9361274719238282\n",
      "Time: 191m 43s (- -192m 36s), Epoch: [1/3], Step: [599/6668], Train Loss: 1.767144012451172\n",
      "Time: 192m 2s (- -192m 16s), Epoch: [1/3], Step: [600/6668], Train Loss: 1.9273345947265625\n",
      "Time: 192m 20s (- -193m 59s), Epoch: [1/3], Step: [601/6668], Train Loss: 2.0140701293945313\n",
      "Time: 192m 37s (- -193m 41s), Epoch: [1/3], Step: [602/6668], Train Loss: 1.7948385620117187\n",
      "Time: 192m 55s (- -193m 24s), Epoch: [1/3], Step: [603/6668], Train Loss: 1.7038673400878905\n",
      "Time: 193m 14s (- -193m 4s), Epoch: [1/3], Step: [604/6668], Train Loss: 1.8470333862304686\n",
      "Time: 193m 32s (- -194m 46s), Epoch: [1/3], Step: [605/6668], Train Loss: 2.000467071533203\n",
      "Time: 193m 50s (- -194m 28s), Epoch: [1/3], Step: [606/6668], Train Loss: 1.5063600158691406\n",
      "Time: 194m 7s (- -194m 11s), Epoch: [1/3], Step: [607/6668], Train Loss: 1.7930221557617188\n",
      "Time: 194m 25s (- -195m 53s), Epoch: [1/3], Step: [608/6668], Train Loss: 1.582572479248047\n",
      "Time: 194m 42s (- -195m 36s), Epoch: [1/3], Step: [609/6668], Train Loss: 1.8478778076171876\n",
      "Time: 195m 1s (- -195m 17s), Epoch: [1/3], Step: [610/6668], Train Loss: 1.7192852783203125\n",
      "Time: 195m 21s (- -196m 57s), Epoch: [1/3], Step: [611/6668], Train Loss: 1.4865547180175782\n",
      "Time: 195m 41s (- -196m 37s), Epoch: [1/3], Step: [612/6668], Train Loss: 1.6377102661132812\n",
      "Time: 196m 1s (- -196m 17s), Epoch: [1/3], Step: [613/6668], Train Loss: 1.9837428283691407\n",
      "Time: 196m 18s (- -196m 0s), Epoch: [1/3], Step: [614/6668], Train Loss: 1.5824382019042968\n",
      "Time: 196m 36s (- -197m 42s), Epoch: [1/3], Step: [615/6668], Train Loss: 1.4583152770996093\n",
      "Time: 196m 55s (- -197m 23s), Epoch: [1/3], Step: [616/6668], Train Loss: 2.005498962402344\n",
      "Time: 197m 15s (- -197m 3s), Epoch: [1/3], Step: [617/6668], Train Loss: 1.6798245239257812\n",
      "Time: 197m 35s (- -198m 43s), Epoch: [1/3], Step: [618/6668], Train Loss: 2.0121705627441404\n",
      "Time: 197m 52s (- -198m 26s), Epoch: [1/3], Step: [619/6668], Train Loss: 1.8374102783203126\n",
      "Time: 198m 10s (- -198m 8s), Epoch: [1/3], Step: [620/6668], Train Loss: 1.537786102294922\n",
      "Time: 198m 30s (- -199m 49s), Epoch: [1/3], Step: [621/6668], Train Loss: 1.9405735778808593\n",
      "Time: 198m 47s (- -199m 31s), Epoch: [1/3], Step: [622/6668], Train Loss: 1.1783226013183594\n",
      "Time: 199m 7s (- -199m 12s), Epoch: [1/3], Step: [623/6668], Train Loss: 1.5309213256835938\n",
      "Time: 199m 24s (- -200m 54s), Epoch: [1/3], Step: [624/6668], Train Loss: 2.0310687255859374\n",
      "Time: 199m 42s (- -200m 36s), Epoch: [1/3], Step: [625/6668], Train Loss: 1.6805795288085938\n",
      "Time: 200m 1s (- -200m 17s), Epoch: [1/3], Step: [626/6668], Train Loss: 1.8313487243652344\n",
      "Time: 200m 20s (- -201m 58s), Epoch: [1/3], Step: [627/6668], Train Loss: 2.1670704650878907\n",
      "Time: 200m 40s (- -201m 38s), Epoch: [1/3], Step: [628/6668], Train Loss: 1.7107511901855468\n",
      "Time: 201m 0s (- -201m 18s), Epoch: [1/3], Step: [629/6668], Train Loss: 1.640201416015625\n",
      "Time: 201m 20s (- -202m 59s), Epoch: [1/3], Step: [630/6668], Train Loss: 1.582684326171875\n",
      "Time: 201m 37s (- -202m 41s), Epoch: [1/3], Step: [631/6668], Train Loss: 1.7238450622558594\n",
      "Time: 201m 55s (- -202m 23s), Epoch: [1/3], Step: [632/6668], Train Loss: 1.3056959533691406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 202m 15s (- -202m 3s), Epoch: [1/3], Step: [633/6668], Train Loss: 1.5234945678710938\n",
      "Time: 202m 34s (- -203m 44s), Epoch: [1/3], Step: [634/6668], Train Loss: 1.8587930297851563\n",
      "Time: 202m 54s (- -203m 24s), Epoch: [1/3], Step: [635/6668], Train Loss: 1.6561166381835937\n",
      "Time: 203m 13s (- -203m 5s), Epoch: [1/3], Step: [636/6668], Train Loss: 1.6528034973144532\n",
      "Time: 203m 33s (- -204m 45s), Epoch: [1/3], Step: [637/6668], Train Loss: 1.9574240112304688\n",
      "Time: 203m 53s (- -204m 26s), Epoch: [1/3], Step: [638/6668], Train Loss: 1.5633697509765625\n",
      "Time: 204m 12s (- -204m 6s), Epoch: [1/3], Step: [639/6668], Train Loss: 2.0767333984375\n",
      "Time: 204m 29s (- -205m 49s), Epoch: [1/3], Step: [640/6668], Train Loss: 1.9312225341796876\n",
      "Time: 204m 49s (- -205m 29s), Epoch: [1/3], Step: [641/6668], Train Loss: 1.8741275024414064\n",
      "Time: 205m 7s (- -205m 11s), Epoch: [1/3], Step: [642/6668], Train Loss: 1.4528022766113282\n",
      "Time: 205m 25s (- -206m 53s), Epoch: [1/3], Step: [643/6668], Train Loss: 2.029856872558594\n",
      "Time: 205m 44s (- -206m 34s), Epoch: [1/3], Step: [644/6668], Train Loss: 1.8473594665527344\n",
      "Time: 206m 4s (- -206m 14s), Epoch: [1/3], Step: [645/6668], Train Loss: 1.9800567626953125\n",
      "Time: 206m 21s (- -207m 57s), Epoch: [1/3], Step: [646/6668], Train Loss: 1.722171630859375\n",
      "Time: 206m 41s (- -207m 38s), Epoch: [1/3], Step: [647/6668], Train Loss: 2.098313903808594\n",
      "Time: 206m 58s (- -207m 20s), Epoch: [1/3], Step: [648/6668], Train Loss: 1.6351263427734375\n",
      "Time: 207m 15s (- -207m 3s), Epoch: [1/3], Step: [649/6668], Train Loss: 1.7904837036132812\n",
      "Time: 207m 34s (- -208m 44s), Epoch: [1/3], Step: [650/6668], Train Loss: 1.695921630859375\n",
      "Time: 207m 52s (- -208m 26s), Epoch: [1/3], Step: [651/6668], Train Loss: 2.014315643310547\n",
      "Time: 208m 9s (- -208m 9s), Epoch: [1/3], Step: [652/6668], Train Loss: 1.6016555786132813\n",
      "Time: 208m 28s (- -209m 50s), Epoch: [1/3], Step: [653/6668], Train Loss: 1.9409152221679689\n",
      "Time: 208m 45s (- -209m 33s), Epoch: [1/3], Step: [654/6668], Train Loss: 2.100372314453125\n",
      "Time: 209m 3s (- -209m 16s), Epoch: [1/3], Step: [655/6668], Train Loss: 1.9820675659179687\n",
      "Time: 209m 20s (- -210m 58s), Epoch: [1/3], Step: [656/6668], Train Loss: 1.5670091247558593\n",
      "Time: 209m 40s (- -210m 38s), Epoch: [1/3], Step: [657/6668], Train Loss: 1.748152313232422\n",
      "Time: 209m 57s (- -210m 21s), Epoch: [1/3], Step: [658/6668], Train Loss: 2.1528001403808594\n",
      "Time: 210m 17s (- -210m 1s), Epoch: [1/3], Step: [659/6668], Train Loss: 1.7213272094726562\n",
      "Time: 210m 36s (- -211m 42s), Epoch: [1/3], Step: [660/6668], Train Loss: 1.6815202331542969\n",
      "Time: 210m 53s (- -211m 25s), Epoch: [1/3], Step: [661/6668], Train Loss: 1.5719749450683593\n",
      "Time: 211m 10s (- -211m 8s), Epoch: [1/3], Step: [662/6668], Train Loss: 1.8153872680664063\n",
      "Time: 211m 30s (- -212m 48s), Epoch: [1/3], Step: [663/6668], Train Loss: 1.5419941711425782\n",
      "Time: 211m 49s (- -212m 29s), Epoch: [1/3], Step: [664/6668], Train Loss: 1.8204135131835937\n",
      "Time: 212m 8s (- -212m 10s), Epoch: [1/3], Step: [665/6668], Train Loss: 1.9075332641601563\n",
      "Time: 212m 28s (- -213m 50s), Epoch: [1/3], Step: [666/6668], Train Loss: 1.5721804809570312\n",
      "Time: 212m 45s (- -213m 33s), Epoch: [1/3], Step: [667/6668], Train Loss: 1.8468820190429687\n",
      "Time: 213m 2s (- -213m 16s), Epoch: [1/3], Step: [668/6668], Train Loss: 1.7482026672363282\n",
      "Time: 213m 20s (- -214m 58s), Epoch: [1/3], Step: [669/6668], Train Loss: 1.6569114685058595\n",
      "Time: 213m 37s (- -214m 41s), Epoch: [1/3], Step: [670/6668], Train Loss: 2.087106475830078\n",
      "Time: 213m 55s (- -214m 23s), Epoch: [1/3], Step: [671/6668], Train Loss: 2.351592254638672\n",
      "Time: 214m 12s (- -214m 6s), Epoch: [1/3], Step: [672/6668], Train Loss: 1.5760868835449218\n",
      "Time: 214m 30s (- -215m 48s), Epoch: [1/3], Step: [673/6668], Train Loss: 1.8819850158691407\n",
      "Time: 214m 49s (- -215m 29s), Epoch: [1/3], Step: [674/6668], Train Loss: 1.5648506164550782\n",
      "Time: 215m 8s (- -215m 10s), Epoch: [1/3], Step: [675/6668], Train Loss: 2.16991943359375\n",
      "Time: 215m 28s (- -216m 50s), Epoch: [1/3], Step: [676/6668], Train Loss: 1.5637490844726563\n",
      "Time: 215m 47s (- -216m 31s), Epoch: [1/3], Step: [677/6668], Train Loss: 2.06298828125\n",
      "Time: 216m 7s (- -216m 11s), Epoch: [1/3], Step: [678/6668], Train Loss: 1.7742294311523437\n",
      "Time: 216m 24s (- -217m 54s), Epoch: [1/3], Step: [679/6668], Train Loss: 1.6372108459472656\n",
      "Time: 216m 42s (- -217m 37s), Epoch: [1/3], Step: [680/6668], Train Loss: 1.754205322265625\n",
      "Time: 217m 1s (- -217m 17s), Epoch: [1/3], Step: [681/6668], Train Loss: 2.0578964233398436\n",
      "Time: 217m 21s (- -218m 57s), Epoch: [1/3], Step: [682/6668], Train Loss: 1.6350949096679688\n",
      "Time: 217m 41s (- -218m 37s), Epoch: [1/3], Step: [683/6668], Train Loss: 1.7405831909179688\n",
      "Time: 218m 0s (- -218m 18s), Epoch: [1/3], Step: [684/6668], Train Loss: 1.715989990234375\n",
      "Time: 218m 20s (- -219m 58s), Epoch: [1/3], Step: [685/6668], Train Loss: 2.280623779296875\n",
      "Time: 218m 37s (- -219m 41s), Epoch: [1/3], Step: [686/6668], Train Loss: 1.77529296875\n",
      "Time: 218m 57s (- -219m 21s), Epoch: [1/3], Step: [687/6668], Train Loss: 1.9229518127441407\n",
      "Time: 219m 17s (- -219m 2s), Epoch: [1/3], Step: [688/6668], Train Loss: 1.7971826171875\n",
      "Time: 219m 34s (- -220m 44s), Epoch: [1/3], Step: [689/6668], Train Loss: 1.6693008422851563\n",
      "Time: 219m 54s (- -220m 24s), Epoch: [1/3], Step: [690/6668], Train Loss: 1.7708303833007812\n",
      "Time: 220m 11s (- -220m 7s), Epoch: [1/3], Step: [691/6668], Train Loss: 1.9316224670410156\n",
      "Time: 220m 31s (- -221m 47s), Epoch: [1/3], Step: [692/6668], Train Loss: 2.0575125122070315\n",
      "Time: 220m 49s (- -221m 29s), Epoch: [1/3], Step: [693/6668], Train Loss: 1.475869140625\n",
      "Time: 221m 9s (- -221m 10s), Epoch: [1/3], Step: [694/6668], Train Loss: 1.7318704223632813\n",
      "Time: 221m 26s (- -222m 52s), Epoch: [1/3], Step: [695/6668], Train Loss: 1.65799072265625\n",
      "Time: 221m 44s (- -222m 34s), Epoch: [1/3], Step: [696/6668], Train Loss: 1.8162799072265625\n",
      "Time: 222m 3s (- -222m 15s), Epoch: [1/3], Step: [697/6668], Train Loss: 1.6268649291992188\n",
      "Time: 222m 20s (- -223m 58s), Epoch: [1/3], Step: [698/6668], Train Loss: 1.5711782836914063\n",
      "Time: 222m 38s (- -223m 40s), Epoch: [1/3], Step: [699/6668], Train Loss: 1.727589111328125\n",
      "Time: 222m 58s (- -223m 21s), Epoch: [1/3], Step: [700/6668], Train Loss: 2.0061180114746096\n",
      "Time: 223m 17s (- -223m 1s), Epoch: [1/3], Step: [701/6668], Train Loss: 1.8154692077636718\n",
      "Time: 223m 35s (- -224m 43s), Epoch: [1/3], Step: [702/6668], Train Loss: 1.8765058898925782\n",
      "Time: 223m 56s (- -224m 22s), Epoch: [1/3], Step: [703/6668], Train Loss: 1.4986235046386718\n",
      "Time: 224m 14s (- -224m 5s), Epoch: [1/3], Step: [704/6668], Train Loss: 1.6521096801757813\n",
      "Time: 224m 33s (- -225m 45s), Epoch: [1/3], Step: [705/6668], Train Loss: 2.052509613037109\n",
      "Time: 224m 52s (- -225m 26s), Epoch: [1/3], Step: [706/6668], Train Loss: 1.9258950805664063\n",
      "Time: 225m 12s (- -225m 6s), Epoch: [1/3], Step: [707/6668], Train Loss: 1.6248936462402344\n",
      "Time: 225m 32s (- -226m 46s), Epoch: [1/3], Step: [708/6668], Train Loss: 1.9285104370117188\n",
      "Time: 225m 53s (- -226m 25s), Epoch: [1/3], Step: [709/6668], Train Loss: 1.4155239868164062\n",
      "Time: 226m 11s (- -226m 8s), Epoch: [1/3], Step: [710/6668], Train Loss: 1.553918914794922\n",
      "Time: 226m 28s (- -227m 50s), Epoch: [1/3], Step: [711/6668], Train Loss: 2.0200672912597657\n",
      "Time: 226m 46s (- -227m 32s), Epoch: [1/3], Step: [712/6668], Train Loss: 1.7427145385742187\n",
      "Time: 227m 5s (- -227m 13s), Epoch: [1/3], Step: [713/6668], Train Loss: 1.7694537353515625\n",
      "Time: 227m 23s (- -228m 55s), Epoch: [1/3], Step: [714/6668], Train Loss: 1.789327392578125\n",
      "Time: 227m 43s (- -228m 36s), Epoch: [1/3], Step: [715/6668], Train Loss: 2.0580766296386717\n",
      "Time: 228m 2s (- -228m 16s), Epoch: [1/3], Step: [716/6668], Train Loss: 1.82379150390625\n",
      "Time: 228m 20s (- -229m 59s), Epoch: [1/3], Step: [717/6668], Train Loss: 1.6732431030273438\n",
      "Time: 228m 39s (- -229m 39s), Epoch: [1/3], Step: [718/6668], Train Loss: 1.5461796569824218\n",
      "Time: 228m 57s (- -229m 21s), Epoch: [1/3], Step: [719/6668], Train Loss: 1.651698760986328\n",
      "Time: 229m 14s (- -229m 4s), Epoch: [1/3], Step: [720/6668], Train Loss: 2.0201271057128904\n",
      "Time: 229m 33s (- -230m 45s), Epoch: [1/3], Step: [721/6668], Train Loss: 1.8817889404296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 229m 51s (- -230m 28s), Epoch: [1/3], Step: [722/6668], Train Loss: 1.540093231201172\n",
      "Time: 230m 8s (- -230m 10s), Epoch: [1/3], Step: [723/6668], Train Loss: 1.5790687561035157\n",
      "Time: 230m 28s (- -231m 51s), Epoch: [1/3], Step: [724/6668], Train Loss: 1.8149642944335938\n",
      "Time: 230m 45s (- -231m 33s), Epoch: [1/3], Step: [725/6668], Train Loss: 1.6631248474121094\n",
      "Time: 231m 3s (- -231m 15s), Epoch: [1/3], Step: [726/6668], Train Loss: 1.6074737548828124\n",
      "Time: 231m 22s (- -232m 56s), Epoch: [1/3], Step: [727/6668], Train Loss: 1.9588938903808595\n",
      "Time: 231m 40s (- -232m 38s), Epoch: [1/3], Step: [728/6668], Train Loss: 1.803288116455078\n",
      "Time: 231m 59s (- -232m 19s), Epoch: [1/3], Step: [729/6668], Train Loss: 1.899918670654297\n",
      "Time: 232m 17s (- -232m 1s), Epoch: [1/3], Step: [730/6668], Train Loss: 2.0578599548339844\n",
      "Time: 232m 34s (- -233m 44s), Epoch: [1/3], Step: [731/6668], Train Loss: 2.036624755859375\n",
      "Time: 232m 52s (- -233m 26s), Epoch: [1/3], Step: [732/6668], Train Loss: 1.332626495361328\n",
      "Time: 233m 11s (- -233m 7s), Epoch: [1/3], Step: [733/6668], Train Loss: 1.7145455932617188\n",
      "Time: 233m 31s (- -234m 47s), Epoch: [1/3], Step: [734/6668], Train Loss: 1.5137705993652344\n",
      "Time: 233m 51s (- -234m 27s), Epoch: [1/3], Step: [735/6668], Train Loss: 1.590804443359375\n",
      "Time: 234m 9s (- -234m 9s), Epoch: [1/3], Step: [736/6668], Train Loss: 1.9372628784179688\n",
      "Time: 234m 26s (- -235m 52s), Epoch: [1/3], Step: [737/6668], Train Loss: 1.626142578125\n",
      "Time: 234m 44s (- -235m 34s), Epoch: [1/3], Step: [738/6668], Train Loss: 2.0202810668945315\n",
      "Time: 235m 4s (- -235m 14s), Epoch: [1/3], Step: [739/6668], Train Loss: 1.7842152404785157\n",
      "Time: 235m 23s (- -236m 55s), Epoch: [1/3], Step: [740/6668], Train Loss: 1.8062416076660157\n",
      "Time: 235m 41s (- -236m 37s), Epoch: [1/3], Step: [741/6668], Train Loss: 1.6744325256347656\n",
      "Time: 235m 59s (- -236m 19s), Epoch: [1/3], Step: [742/6668], Train Loss: 1.7958393859863282\n",
      "Time: 236m 18s (- -236m 0s), Epoch: [1/3], Step: [743/6668], Train Loss: 1.701729278564453\n",
      "Time: 236m 38s (- -237m 40s), Epoch: [1/3], Step: [744/6668], Train Loss: 2.1676907348632812\n",
      "Time: 236m 57s (- -237m 21s), Epoch: [1/3], Step: [745/6668], Train Loss: 2.513460998535156\n",
      "Time: 237m 15s (- -237m 3s), Epoch: [1/3], Step: [746/6668], Train Loss: 1.5064073181152344\n",
      "Time: 237m 34s (- -238m 44s), Epoch: [1/3], Step: [747/6668], Train Loss: 1.8591905212402344\n",
      "Time: 237m 54s (- -238m 24s), Epoch: [1/3], Step: [748/6668], Train Loss: 1.8848980712890624\n",
      "Time: 238m 14s (- -238m 4s), Epoch: [1/3], Step: [749/6668], Train Loss: 1.8409298706054686\n",
      "Time: 238m 35s (- -239m 43s), Epoch: [1/3], Step: [750/6668], Train Loss: 2.191436767578125\n",
      "Time: 238m 53s (- -239m 25s), Epoch: [1/3], Step: [751/6668], Train Loss: 1.8575003051757812\n",
      "Time: 239m 12s (- -239m 6s), Epoch: [1/3], Step: [752/6668], Train Loss: 2.0110205078125\n",
      "Time: 239m 32s (- -240m 46s), Epoch: [1/3], Step: [753/6668], Train Loss: 1.9084518432617188\n",
      "Time: 239m 52s (- -240m 26s), Epoch: [1/3], Step: [754/6668], Train Loss: 2.171933288574219\n",
      "Time: 240m 9s (- -240m 9s), Epoch: [1/3], Step: [755/6668], Train Loss: 1.8373297119140626\n",
      "Time: 240m 29s (- -241m 49s), Epoch: [1/3], Step: [756/6668], Train Loss: 1.8391171264648438\n",
      "Time: 240m 47s (- -241m 32s), Epoch: [1/3], Step: [757/6668], Train Loss: 2.093721923828125\n",
      "Time: 241m 6s (- -241m 12s), Epoch: [1/3], Step: [758/6668], Train Loss: 1.714930419921875\n",
      "Time: 241m 24s (- -242m 54s), Epoch: [1/3], Step: [759/6668], Train Loss: 1.8542970275878907\n",
      "Time: 241m 43s (- -242m 35s), Epoch: [1/3], Step: [760/6668], Train Loss: 1.6446017456054687\n",
      "Time: 242m 1s (- -242m 17s), Epoch: [1/3], Step: [761/6668], Train Loss: 2.0197264099121095\n",
      "Time: 242m 18s (- -242m 0s), Epoch: [1/3], Step: [762/6668], Train Loss: 1.8982803344726562\n",
      "Time: 242m 38s (- -243m 40s), Epoch: [1/3], Step: [763/6668], Train Loss: 1.5778265380859375\n",
      "Time: 242m 57s (- -243m 21s), Epoch: [1/3], Step: [764/6668], Train Loss: 1.5202635192871095\n",
      "Time: 243m 16s (- -243m 2s), Epoch: [1/3], Step: [765/6668], Train Loss: 1.4947381591796876\n",
      "Time: 243m 38s (- -244m 40s), Epoch: [1/3], Step: [766/6668], Train Loss: 2.0235398864746093\n",
      "Time: 243m 58s (- -244m 20s), Epoch: [1/3], Step: [767/6668], Train Loss: 1.6157131958007813\n",
      "Time: 244m 15s (- -244m 3s), Epoch: [1/3], Step: [768/6668], Train Loss: 1.8257957458496095\n",
      "Time: 244m 33s (- -245m 45s), Epoch: [1/3], Step: [769/6668], Train Loss: 1.9201466369628906\n",
      "Time: 244m 52s (- -245m 26s), Epoch: [1/3], Step: [770/6668], Train Loss: 1.7239279174804687\n",
      "Time: 245m 11s (- -245m 7s), Epoch: [1/3], Step: [771/6668], Train Loss: 1.6736297607421875\n",
      "Time: 245m 29s (- -246m 49s), Epoch: [1/3], Step: [772/6668], Train Loss: 2.1703781127929687\n",
      "Time: 245m 46s (- -246m 32s), Epoch: [1/3], Step: [773/6668], Train Loss: 1.6674928283691406\n",
      "Time: 246m 4s (- -246m 14s), Epoch: [1/3], Step: [774/6668], Train Loss: 1.8492829895019531\n",
      "Time: 246m 24s (- -247m 54s), Epoch: [1/3], Step: [775/6668], Train Loss: 1.980050048828125\n",
      "Time: 246m 43s (- -247m 35s), Epoch: [1/3], Step: [776/6668], Train Loss: 2.0672311401367187\n",
      "Time: 247m 1s (- -247m 18s), Epoch: [1/3], Step: [777/6668], Train Loss: 1.7697859191894532\n",
      "Time: 247m 20s (- -248m 58s), Epoch: [1/3], Step: [778/6668], Train Loss: 1.6408808898925782\n",
      "Time: 247m 40s (- -248m 38s), Epoch: [1/3], Step: [779/6668], Train Loss: 2.1170169067382814\n",
      "Time: 247m 59s (- -248m 19s), Epoch: [1/3], Step: [780/6668], Train Loss: 1.9525729370117189\n",
      "Time: 248m 17s (- -248m 1s), Epoch: [1/3], Step: [781/6668], Train Loss: 2.0630615234375\n",
      "Time: 248m 34s (- -249m 44s), Epoch: [1/3], Step: [782/6668], Train Loss: 1.9563088989257813\n",
      "Time: 248m 54s (- -249m 24s), Epoch: [1/3], Step: [783/6668], Train Loss: 1.669074249267578\n",
      "Time: 249m 13s (- -249m 5s), Epoch: [1/3], Step: [784/6668], Train Loss: 1.7766172790527344\n",
      "Time: 249m 31s (- -250m 47s), Epoch: [1/3], Step: [785/6668], Train Loss: 1.7691481018066406\n",
      "Time: 249m 51s (- -250m 27s), Epoch: [1/3], Step: [786/6668], Train Loss: 2.202159118652344\n",
      "Time: 250m 8s (- -250m 10s), Epoch: [1/3], Step: [787/6668], Train Loss: 1.822361602783203\n",
      "Time: 250m 26s (- -251m 52s), Epoch: [1/3], Step: [788/6668], Train Loss: 2.1715980529785157\n",
      "Time: 250m 45s (- -251m 33s), Epoch: [1/3], Step: [789/6668], Train Loss: 1.7435795593261718\n",
      "Time: 251m 4s (- -251m 14s), Epoch: [1/3], Step: [790/6668], Train Loss: 2.4477735900878907\n",
      "Time: 251m 22s (- -252m 56s), Epoch: [1/3], Step: [791/6668], Train Loss: 2.2010174560546876\n",
      "Time: 251m 40s (- -252m 38s), Epoch: [1/3], Step: [792/6668], Train Loss: 1.4923410034179687\n",
      "Time: 252m 0s (- -252m 18s), Epoch: [1/3], Step: [793/6668], Train Loss: 1.744593505859375\n",
      "Time: 252m 19s (- -253m 59s), Epoch: [1/3], Step: [794/6668], Train Loss: 1.7919265747070312\n",
      "Time: 252m 39s (- -253m 39s), Epoch: [1/3], Step: [795/6668], Train Loss: 1.9339413452148437\n",
      "Time: 252m 59s (- -253m 19s), Epoch: [1/3], Step: [796/6668], Train Loss: 1.5773342895507811\n",
      "Time: 253m 16s (- -253m 2s), Epoch: [1/3], Step: [797/6668], Train Loss: 1.5863201904296875\n",
      "Time: 253m 36s (- -254m 42s), Epoch: [1/3], Step: [798/6668], Train Loss: 1.8587841796875\n",
      "Time: 253m 54s (- -254m 24s), Epoch: [1/3], Step: [799/6668], Train Loss: 1.982296142578125\n",
      "Time: 254m 13s (- -254m 5s), Epoch: [1/3], Step: [800/6668], Train Loss: 1.5952752685546876\n",
      "Time: 254m 33s (- -255m 45s), Epoch: [1/3], Step: [801/6668], Train Loss: 2.087118682861328\n",
      "Time: 254m 50s (- -255m 28s), Epoch: [1/3], Step: [802/6668], Train Loss: 1.9872158813476561\n",
      "Time: 255m 10s (- -255m 8s), Epoch: [1/3], Step: [803/6668], Train Loss: 1.5659458923339844\n",
      "Time: 255m 30s (- -256m 48s), Epoch: [1/3], Step: [804/6668], Train Loss: 1.7632075500488282\n",
      "Time: 255m 49s (- -256m 29s), Epoch: [1/3], Step: [805/6668], Train Loss: 1.7733453369140626\n",
      "Time: 256m 6s (- -256m 12s), Epoch: [1/3], Step: [806/6668], Train Loss: 2.1781271362304686\n",
      "Time: 256m 26s (- -257m 52s), Epoch: [1/3], Step: [807/6668], Train Loss: 1.6512156677246095\n",
      "Time: 256m 43s (- -257m 35s), Epoch: [1/3], Step: [808/6668], Train Loss: 1.5479722595214844\n",
      "Time: 257m 1s (- -257m 17s), Epoch: [1/3], Step: [809/6668], Train Loss: 2.3574787902832033\n",
      "Time: 257m 20s (- -258m 58s), Epoch: [1/3], Step: [810/6668], Train Loss: 1.6669677734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 257m 38s (- -258m 40s), Epoch: [1/3], Step: [811/6668], Train Loss: 2.0038180541992188\n",
      "Time: 257m 56s (- -258m 22s), Epoch: [1/3], Step: [812/6668], Train Loss: 2.1654908752441404\n",
      "Time: 258m 13s (- -258m 5s), Epoch: [1/3], Step: [813/6668], Train Loss: 1.6643614196777343\n",
      "Time: 258m 33s (- -259m 45s), Epoch: [1/3], Step: [814/6668], Train Loss: 1.6232528686523438\n",
      "Time: 258m 52s (- -259m 26s), Epoch: [1/3], Step: [815/6668], Train Loss: 1.8224990844726563\n",
      "Time: 259m 10s (- -259m 9s), Epoch: [1/3], Step: [816/6668], Train Loss: 1.7843804931640626\n",
      "Time: 259m 29s (- -260m 49s), Epoch: [1/3], Step: [817/6668], Train Loss: 2.010695495605469\n",
      "Time: 259m 49s (- -260m 29s), Epoch: [1/3], Step: [818/6668], Train Loss: 2.4595600891113283\n",
      "Time: 260m 9s (- -260m 9s), Epoch: [1/3], Step: [819/6668], Train Loss: 1.9231622314453125\n",
      "Time: 260m 26s (- -261m 52s), Epoch: [1/3], Step: [820/6668], Train Loss: 1.7854887390136718\n",
      "Time: 260m 46s (- -261m 32s), Epoch: [1/3], Step: [821/6668], Train Loss: 1.6920744323730468\n",
      "Time: 261m 3s (- -261m 15s), Epoch: [1/3], Step: [822/6668], Train Loss: 1.7798095703125\n",
      "Time: 261m 23s (- -262m 55s), Epoch: [1/3], Step: [823/6668], Train Loss: 1.5783828735351562\n",
      "Time: 261m 42s (- -262m 36s), Epoch: [1/3], Step: [824/6668], Train Loss: 1.8376152038574218\n",
      "Time: 262m 2s (- -262m 16s), Epoch: [1/3], Step: [825/6668], Train Loss: 2.2576699829101563\n",
      "Time: 262m 20s (- -263m 58s), Epoch: [1/3], Step: [826/6668], Train Loss: 1.9050131225585938\n",
      "Time: 262m 39s (- -263m 39s), Epoch: [1/3], Step: [827/6668], Train Loss: 2.4469168090820315\n",
      "Time: 262m 59s (- -263m 19s), Epoch: [1/3], Step: [828/6668], Train Loss: 1.8366441345214843\n",
      "Time: 263m 16s (- -263m 2s), Epoch: [1/3], Step: [829/6668], Train Loss: 1.7380825805664062\n",
      "Time: 263m 34s (- -264m 44s), Epoch: [1/3], Step: [830/6668], Train Loss: 1.6521426391601564\n",
      "Time: 263m 54s (- -264m 24s), Epoch: [1/3], Step: [831/6668], Train Loss: 1.6094148254394531\n",
      "Time: 264m 12s (- -264m 6s), Epoch: [1/3], Step: [832/6668], Train Loss: 1.8357420349121094\n",
      "Time: 264m 31s (- -265m 47s), Epoch: [1/3], Step: [833/6668], Train Loss: 2.2439707946777343\n",
      "Time: 264m 49s (- -265m 29s), Epoch: [1/3], Step: [834/6668], Train Loss: 1.9058990478515625\n",
      "Time: 265m 6s (- -265m 12s), Epoch: [1/3], Step: [835/6668], Train Loss: 1.9761869812011719\n",
      "Time: 265m 26s (- -266m 52s), Epoch: [1/3], Step: [836/6668], Train Loss: 1.90999755859375\n",
      "Time: 265m 44s (- -266m 34s), Epoch: [1/3], Step: [837/6668], Train Loss: 1.6231192016601563\n",
      "Time: 266m 1s (- -266m 17s), Epoch: [1/3], Step: [838/6668], Train Loss: 1.8096881103515625\n",
      "Time: 266m 19s (- -267m 59s), Epoch: [1/3], Step: [839/6668], Train Loss: 2.2816195678710938\n",
      "Time: 266m 39s (- -267m 39s), Epoch: [1/3], Step: [840/6668], Train Loss: 1.2550181579589843\n",
      "Time: 266m 56s (- -267m 22s), Epoch: [1/3], Step: [841/6668], Train Loss: 1.7900149536132812\n",
      "Time: 267m 16s (- -267m 2s), Epoch: [1/3], Step: [842/6668], Train Loss: 2.009337158203125\n",
      "Time: 267m 35s (- -268m 43s), Epoch: [1/3], Step: [843/6668], Train Loss: 1.592701416015625\n",
      "Time: 267m 53s (- -268m 25s), Epoch: [1/3], Step: [844/6668], Train Loss: 1.7225863647460937\n",
      "Time: 268m 10s (- -268m 8s), Epoch: [1/3], Step: [845/6668], Train Loss: 2.2978309631347655\n",
      "Time: 268m 28s (- -269m 50s), Epoch: [1/3], Step: [846/6668], Train Loss: 1.9740623474121093\n",
      "Time: 268m 48s (- -269m 30s), Epoch: [1/3], Step: [847/6668], Train Loss: 1.3199017333984375\n",
      "Time: 269m 7s (- -269m 11s), Epoch: [1/3], Step: [848/6668], Train Loss: 1.5023321533203124\n",
      "Time: 269m 27s (- -270m 51s), Epoch: [1/3], Step: [849/6668], Train Loss: 1.7403886413574219\n",
      "Time: 269m 47s (- -270m 31s), Epoch: [1/3], Step: [850/6668], Train Loss: 1.4701821899414063\n",
      "Time: 270m 5s (- -270m 13s), Epoch: [1/3], Step: [851/6668], Train Loss: 1.7296990966796875\n",
      "Time: 270m 24s (- -271m 54s), Epoch: [1/3], Step: [852/6668], Train Loss: 1.5898236083984374\n",
      "Time: 270m 41s (- -271m 37s), Epoch: [1/3], Step: [853/6668], Train Loss: 2.16211181640625\n",
      "Time: 270m 59s (- -271m 19s), Epoch: [1/3], Step: [854/6668], Train Loss: 1.5743290710449218\n",
      "Time: 271m 18s (- -271m 0s), Epoch: [1/3], Step: [855/6668], Train Loss: 1.7483631896972656\n",
      "Time: 271m 35s (- -272m 43s), Epoch: [1/3], Step: [856/6668], Train Loss: 1.6660914611816406\n",
      "Time: 271m 53s (- -272m 25s), Epoch: [1/3], Step: [857/6668], Train Loss: 2.070884246826172\n",
      "Time: 272m 13s (- -272m 5s), Epoch: [1/3], Step: [858/6668], Train Loss: 2.2720155334472656\n",
      "Time: 272m 30s (- -273m 48s), Epoch: [1/3], Step: [859/6668], Train Loss: 1.636064453125\n",
      "Time: 272m 50s (- -273m 28s), Epoch: [1/3], Step: [860/6668], Train Loss: 2.1649513244628906\n",
      "Time: 273m 7s (- -273m 11s), Epoch: [1/3], Step: [861/6668], Train Loss: 2.2369171142578126\n",
      "Time: 273m 27s (- -274m 51s), Epoch: [1/3], Step: [862/6668], Train Loss: 1.7577995300292968\n",
      "Time: 273m 48s (- -274m 30s), Epoch: [1/3], Step: [863/6668], Train Loss: 2.027716064453125\n",
      "Time: 274m 5s (- -274m 13s), Epoch: [1/3], Step: [864/6668], Train Loss: 1.6861158752441405\n",
      "Time: 274m 25s (- -275m 53s), Epoch: [1/3], Step: [865/6668], Train Loss: 1.886900634765625\n",
      "Time: 274m 42s (- -275m 36s), Epoch: [1/3], Step: [866/6668], Train Loss: 2.194996643066406\n",
      "Time: 275m 2s (- -275m 16s), Epoch: [1/3], Step: [867/6668], Train Loss: 2.0706219482421875\n",
      "Time: 275m 19s (- -276m 59s), Epoch: [1/3], Step: [868/6668], Train Loss: 1.855625762939453\n",
      "Time: 275m 39s (- -276m 39s), Epoch: [1/3], Step: [869/6668], Train Loss: 2.1127005004882813\n",
      "Time: 275m 57s (- -276m 21s), Epoch: [1/3], Step: [870/6668], Train Loss: 2.031619873046875\n",
      "Time: 276m 14s (- -276m 4s), Epoch: [1/3], Step: [871/6668], Train Loss: 1.8207722473144532\n",
      "Time: 276m 33s (- -277m 45s), Epoch: [1/3], Step: [872/6668], Train Loss: 1.971153564453125\n",
      "Time: 276m 53s (- -277m 25s), Epoch: [1/3], Step: [873/6668], Train Loss: 1.7033982849121094\n",
      "Time: 277m 10s (- -277m 8s), Epoch: [1/3], Step: [874/6668], Train Loss: 1.7918751525878907\n",
      "Time: 277m 28s (- -278m 50s), Epoch: [1/3], Step: [875/6668], Train Loss: 1.9819683837890625\n",
      "Time: 277m 48s (- -278m 30s), Epoch: [1/3], Step: [876/6668], Train Loss: 2.2410549926757812\n",
      "Time: 278m 7s (- -278m 11s), Epoch: [1/3], Step: [877/6668], Train Loss: 1.79115478515625\n",
      "Time: 278m 27s (- -279m 51s), Epoch: [1/3], Step: [878/6668], Train Loss: 2.292613220214844\n",
      "Time: 278m 48s (- -279m 30s), Epoch: [1/3], Step: [879/6668], Train Loss: 1.6260162353515626\n",
      "Time: 279m 8s (- -279m 10s), Epoch: [1/3], Step: [880/6668], Train Loss: 2.048340148925781\n",
      "Time: 279m 28s (- -280m 50s), Epoch: [1/3], Step: [881/6668], Train Loss: 2.068015441894531\n",
      "Time: 279m 48s (- -280m 30s), Epoch: [1/3], Step: [882/6668], Train Loss: 1.7183329772949218\n",
      "Time: 280m 8s (- -280m 10s), Epoch: [1/3], Step: [883/6668], Train Loss: 1.85680419921875\n",
      "Time: 280m 25s (- -281m 53s), Epoch: [1/3], Step: [884/6668], Train Loss: 2.2081570434570312\n",
      "Time: 280m 43s (- -281m 35s), Epoch: [1/3], Step: [885/6668], Train Loss: 1.9050430297851562\n",
      "Time: 281m 2s (- -281m 16s), Epoch: [1/3], Step: [886/6668], Train Loss: 1.6579400634765624\n",
      "Time: 281m 23s (- -282m 56s), Epoch: [1/3], Step: [887/6668], Train Loss: 1.8378988647460937\n",
      "Time: 281m 42s (- -282m 36s), Epoch: [1/3], Step: [888/6668], Train Loss: 1.9182322692871094\n",
      "Time: 282m 2s (- -282m 16s), Epoch: [1/3], Step: [889/6668], Train Loss: 1.4467475891113282\n",
      "Time: 282m 22s (- -283m 56s), Epoch: [1/3], Step: [890/6668], Train Loss: 1.8305279541015624\n",
      "Time: 282m 39s (- -283m 39s), Epoch: [1/3], Step: [891/6668], Train Loss: 2.0615890502929686\n",
      "Time: 282m 58s (- -283m 20s), Epoch: [1/3], Step: [892/6668], Train Loss: 1.717667999267578\n",
      "Time: 283m 18s (- -283m 0s), Epoch: [1/3], Step: [893/6668], Train Loss: 1.2705665588378907\n",
      "Time: 283m 36s (- -284m 42s), Epoch: [1/3], Step: [894/6668], Train Loss: 2.2081813049316406\n",
      "Time: 283m 53s (- -284m 25s), Epoch: [1/3], Step: [895/6668], Train Loss: 2.3112974548339844\n",
      "Time: 284m 11s (- -284m 7s), Epoch: [1/3], Step: [896/6668], Train Loss: 1.9736442565917969\n",
      "Time: 284m 30s (- -285m 48s), Epoch: [1/3], Step: [897/6668], Train Loss: 1.6968203735351564\n",
      "Time: 284m 50s (- -285m 28s), Epoch: [1/3], Step: [898/6668], Train Loss: 1.6328187561035157\n",
      "Time: 285m 7s (- -285m 11s), Epoch: [1/3], Step: [899/6668], Train Loss: 1.9825662231445313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 285m 27s (- -286m 52s), Epoch: [1/3], Step: [900/6668], Train Loss: 1.9199696350097657\n",
      "Time: 285m 44s (- -286m 34s), Epoch: [1/3], Step: [901/6668], Train Loss: 2.233931121826172\n",
      "Time: 286m 3s (- -286m 15s), Epoch: [1/3], Step: [902/6668], Train Loss: 2.233282012939453\n",
      "Time: 286m 23s (- -287m 55s), Epoch: [1/3], Step: [903/6668], Train Loss: 1.6466780090332032\n",
      "Time: 286m 43s (- -287m 35s), Epoch: [1/3], Step: [904/6668], Train Loss: 2.116475372314453\n",
      "Time: 287m 3s (- -287m 16s), Epoch: [1/3], Step: [905/6668], Train Loss: 2.272687530517578\n",
      "Time: 287m 20s (- -288m 58s), Epoch: [1/3], Step: [906/6668], Train Loss: 1.958985595703125\n",
      "Time: 287m 40s (- -288m 38s), Epoch: [1/3], Step: [907/6668], Train Loss: 1.6639521789550782\n",
      "Time: 287m 58s (- -288m 20s), Epoch: [1/3], Step: [908/6668], Train Loss: 1.603085479736328\n",
      "Time: 288m 17s (- -288m 1s), Epoch: [1/3], Step: [909/6668], Train Loss: 1.8957232666015624\n",
      "Time: 288m 36s (- -289m 42s), Epoch: [1/3], Step: [910/6668], Train Loss: 1.8104896545410156\n",
      "Time: 288m 56s (- -289m 22s), Epoch: [1/3], Step: [911/6668], Train Loss: 1.498603057861328\n",
      "Time: 289m 16s (- -289m 2s), Epoch: [1/3], Step: [912/6668], Train Loss: 1.578721466064453\n",
      "Time: 289m 34s (- -290m 44s), Epoch: [1/3], Step: [913/6668], Train Loss: 2.5866949462890627\n",
      "Time: 289m 51s (- -290m 27s), Epoch: [1/3], Step: [914/6668], Train Loss: 1.7400881958007812\n",
      "Time: 290m 8s (- -290m 10s), Epoch: [1/3], Step: [915/6668], Train Loss: 1.8496534729003906\n",
      "Time: 290m 26s (- -291m 52s), Epoch: [1/3], Step: [916/6668], Train Loss: 1.9874029541015625\n",
      "Time: 290m 45s (- -291m 33s), Epoch: [1/3], Step: [917/6668], Train Loss: 1.92277099609375\n",
      "Time: 291m 5s (- -291m 14s), Epoch: [1/3], Step: [918/6668], Train Loss: 2.302961730957031\n",
      "Time: 291m 22s (- -292m 56s), Epoch: [1/3], Step: [919/6668], Train Loss: 1.6877867126464843\n",
      "Time: 291m 42s (- -292m 36s), Epoch: [1/3], Step: [920/6668], Train Loss: 1.8530918884277343\n",
      "Time: 291m 59s (- -292m 19s), Epoch: [1/3], Step: [921/6668], Train Loss: 1.7422357177734376\n",
      "Time: 292m 19s (- -293m 59s), Epoch: [1/3], Step: [922/6668], Train Loss: 2.1925331115722657\n",
      "Time: 292m 38s (- -293m 40s), Epoch: [1/3], Step: [923/6668], Train Loss: 1.8680712890625\n",
      "Time: 292m 57s (- -293m 21s), Epoch: [1/3], Step: [924/6668], Train Loss: 1.3910169982910157\n",
      "Time: 293m 15s (- -293m 3s), Epoch: [1/3], Step: [925/6668], Train Loss: 1.709863739013672\n",
      "Time: 293m 34s (- -294m 44s), Epoch: [1/3], Step: [926/6668], Train Loss: 2.4221311950683595\n",
      "Time: 293m 52s (- -294m 26s), Epoch: [1/3], Step: [927/6668], Train Loss: 2.275543518066406\n",
      "Time: 294m 12s (- -294m 6s), Epoch: [1/3], Step: [928/6668], Train Loss: 1.8395135498046875\n",
      "Time: 294m 29s (- -295m 49s), Epoch: [1/3], Step: [929/6668], Train Loss: 1.9797050476074218\n",
      "Time: 294m 48s (- -295m 30s), Epoch: [1/3], Step: [930/6668], Train Loss: 2.471912078857422\n",
      "Time: 295m 6s (- -295m 12s), Epoch: [1/3], Step: [931/6668], Train Loss: 1.88718994140625\n",
      "Time: 295m 25s (- -296m 53s), Epoch: [1/3], Step: [932/6668], Train Loss: 2.066134033203125\n",
      "Time: 295m 45s (- -296m 33s), Epoch: [1/3], Step: [933/6668], Train Loss: 2.0986219787597657\n",
      "Time: 296m 5s (- -296m 13s), Epoch: [1/3], Step: [934/6668], Train Loss: 2.1670266723632814\n",
      "Time: 296m 22s (- -297m 56s), Epoch: [1/3], Step: [935/6668], Train Loss: 1.8827644348144532\n",
      "Time: 296m 40s (- -297m 38s), Epoch: [1/3], Step: [936/6668], Train Loss: 1.9922276306152344\n",
      "Time: 296m 59s (- -297m 19s), Epoch: [1/3], Step: [937/6668], Train Loss: 2.380535430908203\n",
      "Time: 297m 19s (- -298m 59s), Epoch: [1/3], Step: [938/6668], Train Loss: 2.0417190551757813\n",
      "Time: 297m 37s (- -298m 41s), Epoch: [1/3], Step: [939/6668], Train Loss: 1.6320155334472657\n",
      "Time: 297m 54s (- -298m 24s), Epoch: [1/3], Step: [940/6668], Train Loss: 1.8897216796875\n",
      "Time: 298m 12s (- -298m 6s), Epoch: [1/3], Step: [941/6668], Train Loss: 2.442061462402344\n",
      "Time: 298m 33s (- -299m 45s), Epoch: [1/3], Step: [942/6668], Train Loss: 1.9028898620605468\n",
      "Time: 298m 52s (- -299m 26s), Epoch: [1/3], Step: [943/6668], Train Loss: 1.8547390747070311\n",
      "Time: 299m 10s (- -299m 8s), Epoch: [1/3], Step: [944/6668], Train Loss: 1.8357916259765625\n",
      "Time: 299m 27s (- -300m 51s), Epoch: [1/3], Step: [945/6668], Train Loss: 1.9795584106445312\n",
      "Time: 299m 47s (- -300m 31s), Epoch: [1/3], Step: [946/6668], Train Loss: 1.8988861083984374\n",
      "Time: 300m 4s (- -300m 14s), Epoch: [1/3], Step: [947/6668], Train Loss: 1.5318087768554687\n",
      "Time: 300m 22s (- -301m 56s), Epoch: [1/3], Step: [948/6668], Train Loss: 1.8982766723632813\n",
      "Time: 300m 41s (- -301m 37s), Epoch: [1/3], Step: [949/6668], Train Loss: 1.8127601623535157\n",
      "Time: 300m 58s (- -301m 20s), Epoch: [1/3], Step: [950/6668], Train Loss: 2.180821838378906\n",
      "Time: 301m 16s (- -301m 2s), Epoch: [1/3], Step: [951/6668], Train Loss: 1.741815643310547\n",
      "Time: 301m 33s (- -302m 45s), Epoch: [1/3], Step: [952/6668], Train Loss: 1.969346160888672\n",
      "Time: 301m 53s (- -302m 25s), Epoch: [1/3], Step: [953/6668], Train Loss: 2.2894227600097654\n",
      "Time: 302m 10s (- -302m 8s), Epoch: [1/3], Step: [954/6668], Train Loss: 1.7448617553710937\n",
      "Time: 302m 28s (- -303m 50s), Epoch: [1/3], Step: [955/6668], Train Loss: 1.9139015197753906\n",
      "Time: 302m 47s (- -303m 31s), Epoch: [1/3], Step: [956/6668], Train Loss: 1.6305816650390625\n",
      "Time: 303m 6s (- -303m 12s), Epoch: [1/3], Step: [957/6668], Train Loss: 1.9496832275390625\n",
      "Time: 303m 28s (- -304m 50s), Epoch: [1/3], Step: [958/6668], Train Loss: 1.8366152954101562\n",
      "Time: 303m 50s (- -304m 28s), Epoch: [1/3], Step: [959/6668], Train Loss: 2.1460061645507813\n",
      "Time: 304m 10s (- -304m 8s), Epoch: [1/3], Step: [960/6668], Train Loss: 2.229929962158203\n",
      "Time: 304m 28s (- -305m 50s), Epoch: [1/3], Step: [961/6668], Train Loss: 1.813054656982422\n",
      "Time: 304m 48s (- -305m 30s), Epoch: [1/3], Step: [962/6668], Train Loss: 1.5844972229003906\n",
      "Time: 305m 5s (- -305m 13s), Epoch: [1/3], Step: [963/6668], Train Loss: 1.486566162109375\n",
      "Time: 305m 26s (- -306m 52s), Epoch: [1/3], Step: [964/6668], Train Loss: 1.6829168701171875\n",
      "Time: 305m 44s (- -306m 34s), Epoch: [1/3], Step: [965/6668], Train Loss: 2.2925054931640627\n",
      "Time: 306m 4s (- -306m 14s), Epoch: [1/3], Step: [966/6668], Train Loss: 2.302415313720703\n",
      "Time: 306m 21s (- -307m 57s), Epoch: [1/3], Step: [967/6668], Train Loss: 2.045850982666016\n",
      "Time: 306m 40s (- -307m 38s), Epoch: [1/3], Step: [968/6668], Train Loss: 1.7935995483398437\n",
      "Time: 306m 57s (- -307m 21s), Epoch: [1/3], Step: [969/6668], Train Loss: 1.8518162536621094\n",
      "Time: 307m 17s (- -307m 1s), Epoch: [1/3], Step: [970/6668], Train Loss: 1.938013153076172\n",
      "Time: 307m 36s (- -308m 42s), Epoch: [1/3], Step: [971/6668], Train Loss: 2.9300851440429687\n",
      "Time: 307m 53s (- -308m 25s), Epoch: [1/3], Step: [972/6668], Train Loss: 1.80049072265625\n",
      "Time: 308m 10s (- -308m 8s), Epoch: [1/3], Step: [973/6668], Train Loss: 1.8091059875488282\n",
      "Time: 308m 28s (- -309m 50s), Epoch: [1/3], Step: [974/6668], Train Loss: 1.594445037841797\n",
      "Time: 308m 45s (- -309m 33s), Epoch: [1/3], Step: [975/6668], Train Loss: 1.6687973022460938\n",
      "Time: 309m 4s (- -309m 14s), Epoch: [1/3], Step: [976/6668], Train Loss: 2.268627471923828\n",
      "Time: 309m 22s (- -310m 56s), Epoch: [1/3], Step: [977/6668], Train Loss: 1.456679229736328\n",
      "Time: 309m 42s (- -310m 36s), Epoch: [1/3], Step: [978/6668], Train Loss: 1.967074432373047\n",
      "Time: 310m 1s (- -310m 17s), Epoch: [1/3], Step: [979/6668], Train Loss: 2.1933229064941404\n",
      "Time: 310m 19s (- -311m 59s), Epoch: [1/3], Step: [980/6668], Train Loss: 1.7454437255859374\n",
      "Time: 310m 38s (- -311m 40s), Epoch: [1/3], Step: [981/6668], Train Loss: 1.7234822082519532\n",
      "Time: 310m 55s (- -311m 23s), Epoch: [1/3], Step: [982/6668], Train Loss: 1.744645538330078\n",
      "Time: 311m 15s (- -311m 3s), Epoch: [1/3], Step: [983/6668], Train Loss: 1.4922050476074218\n",
      "Time: 311m 35s (- -312m 43s), Epoch: [1/3], Step: [984/6668], Train Loss: 1.7719195556640626\n",
      "Time: 311m 52s (- -312m 26s), Epoch: [1/3], Step: [985/6668], Train Loss: 1.5918783569335937\n",
      "Time: 312m 9s (- -312m 9s), Epoch: [1/3], Step: [986/6668], Train Loss: 1.8269712829589844\n",
      "Time: 312m 27s (- -313m 51s), Epoch: [1/3], Step: [987/6668], Train Loss: 2.208970794677734\n",
      "Time: 312m 44s (- -313m 34s), Epoch: [1/3], Step: [988/6668], Train Loss: 1.7789207458496095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 313m 2s (- -313m 16s), Epoch: [1/3], Step: [989/6668], Train Loss: 1.9140226745605469\n",
      "Time: 313m 21s (- -314m 57s), Epoch: [1/3], Step: [990/6668], Train Loss: 2.116273193359375\n",
      "Time: 313m 41s (- -314m 37s), Epoch: [1/3], Step: [991/6668], Train Loss: 2.1886410522460937\n",
      "Time: 313m 58s (- -314m 20s), Epoch: [1/3], Step: [992/6668], Train Loss: 2.04521728515625\n",
      "Time: 314m 16s (- -314m 2s), Epoch: [1/3], Step: [993/6668], Train Loss: 1.7612115478515624\n",
      "Time: 314m 35s (- -315m 43s), Epoch: [1/3], Step: [994/6668], Train Loss: 1.7063851928710938\n",
      "Time: 314m 53s (- -315m 25s), Epoch: [1/3], Step: [995/6668], Train Loss: 1.6263760375976561\n",
      "Time: 315m 12s (- -315m 6s), Epoch: [1/3], Step: [996/6668], Train Loss: 1.7301412963867187\n",
      "Time: 315m 32s (- -316m 46s), Epoch: [1/3], Step: [997/6668], Train Loss: 1.8945343017578125\n",
      "Time: 315m 50s (- -316m 28s), Epoch: [1/3], Step: [998/6668], Train Loss: 1.56754150390625\n",
      "Time: 316m 7s (- -316m 11s), Epoch: [1/3], Step: [999/6668], Train Loss: 2.062333068847656\n",
      "Time: 316m 25s (- -317m 53s), Epoch: [1/3], Step: [1000/6668], Train Loss: 2.4511956787109375\n",
      "Time: 316m 44s (- -317m 34s), Epoch: [1/3], Step: [1001/6668], Train Loss: 1.4468626403808593\n",
      "Time: 317m 2s (- -317m 16s), Epoch: [1/3], Step: [1002/6668], Train Loss: 2.122235870361328\n",
      "Time: 317m 21s (- -318m 57s), Epoch: [1/3], Step: [1003/6668], Train Loss: 2.0842625427246095\n",
      "Time: 317m 41s (- -318m 37s), Epoch: [1/3], Step: [1004/6668], Train Loss: 2.1602880859375\n",
      "Time: 317m 58s (- -318m 20s), Epoch: [1/3], Step: [1005/6668], Train Loss: 1.9979681396484374\n",
      "Time: 318m 18s (- -318m 0s), Epoch: [1/3], Step: [1006/6668], Train Loss: 1.5757553100585937\n",
      "Time: 318m 37s (- -319m 41s), Epoch: [1/3], Step: [1007/6668], Train Loss: 1.8088265991210937\n",
      "Time: 318m 59s (- -319m 19s), Epoch: [1/3], Step: [1008/6668], Train Loss: 1.9288296508789062\n",
      "Time: 319m 17s (- -319m 1s), Epoch: [1/3], Step: [1009/6668], Train Loss: 1.699736328125\n",
      "Time: 319m 37s (- -320m 41s), Epoch: [1/3], Step: [1010/6668], Train Loss: 2.098397064208984\n",
      "Time: 319m 56s (- -320m 22s), Epoch: [1/3], Step: [1011/6668], Train Loss: 1.8616856384277343\n",
      "Time: 320m 14s (- -320m 4s), Epoch: [1/3], Step: [1012/6668], Train Loss: 1.7140386962890626\n",
      "Time: 320m 33s (- -321m 45s), Epoch: [1/3], Step: [1013/6668], Train Loss: 1.9748820495605468\n",
      "Time: 320m 51s (- -321m 27s), Epoch: [1/3], Step: [1014/6668], Train Loss: 1.8178916931152345\n",
      "Time: 321m 10s (- -321m 8s), Epoch: [1/3], Step: [1015/6668], Train Loss: 1.7192857360839844\n",
      "Time: 321m 31s (- -322m 47s), Epoch: [1/3], Step: [1016/6668], Train Loss: 2.4800346374511717\n",
      "Time: 321m 50s (- -322m 28s), Epoch: [1/3], Step: [1017/6668], Train Loss: 2.1573112487792967\n",
      "Time: 322m 8s (- -322m 10s), Epoch: [1/3], Step: [1018/6668], Train Loss: 1.866246337890625\n",
      "Time: 322m 25s (- -323m 53s), Epoch: [1/3], Step: [1019/6668], Train Loss: 1.9338052368164063\n",
      "Time: 322m 45s (- -323m 33s), Epoch: [1/3], Step: [1020/6668], Train Loss: 1.8546885681152343\n",
      "Time: 323m 2s (- -323m 16s), Epoch: [1/3], Step: [1021/6668], Train Loss: 2.034757080078125\n",
      "Time: 323m 20s (- -324m 58s), Epoch: [1/3], Step: [1022/6668], Train Loss: 1.6091970825195312\n",
      "Time: 323m 40s (- -324m 38s), Epoch: [1/3], Step: [1023/6668], Train Loss: 2.041676788330078\n",
      "Time: 324m 0s (- -324m 18s), Epoch: [1/3], Step: [1024/6668], Train Loss: 1.706328125\n",
      "Time: 324m 17s (- -324m 1s), Epoch: [1/3], Step: [1025/6668], Train Loss: 1.9265335083007813\n",
      "Time: 324m 35s (- -325m 43s), Epoch: [1/3], Step: [1026/6668], Train Loss: 1.8852687072753906\n",
      "Time: 324m 52s (- -325m 26s), Epoch: [1/3], Step: [1027/6668], Train Loss: 2.3916880798339846\n",
      "Time: 325m 12s (- -325m 6s), Epoch: [1/3], Step: [1028/6668], Train Loss: 1.7126872253417968\n",
      "Time: 325m 32s (- -326m 46s), Epoch: [1/3], Step: [1029/6668], Train Loss: 2.1694386291503904\n",
      "Time: 325m 51s (- -326m 26s), Epoch: [1/3], Step: [1030/6668], Train Loss: 1.9368461608886718\n",
      "Time: 326m 9s (- -326m 9s), Epoch: [1/3], Step: [1031/6668], Train Loss: 1.9837205505371094\n",
      "Time: 326m 26s (- -327m 52s), Epoch: [1/3], Step: [1032/6668], Train Loss: 1.5854147338867188\n",
      "Time: 326m 46s (- -327m 32s), Epoch: [1/3], Step: [1033/6668], Train Loss: 2.289225769042969\n",
      "Time: 327m 3s (- -327m 15s), Epoch: [1/3], Step: [1034/6668], Train Loss: 2.009743347167969\n",
      "Time: 327m 23s (- -328m 55s), Epoch: [1/3], Step: [1035/6668], Train Loss: 1.5320936584472655\n",
      "Time: 327m 43s (- -328m 35s), Epoch: [1/3], Step: [1036/6668], Train Loss: 1.8178399658203126\n",
      "Time: 328m 0s (- -328m 18s), Epoch: [1/3], Step: [1037/6668], Train Loss: 1.5770037841796876\n",
      "Time: 328m 20s (- -329m 58s), Epoch: [1/3], Step: [1038/6668], Train Loss: 1.793543701171875\n",
      "Time: 328m 39s (- -329m 39s), Epoch: [1/3], Step: [1039/6668], Train Loss: 1.6693426513671874\n",
      "Time: 328m 56s (- -329m 22s), Epoch: [1/3], Step: [1040/6668], Train Loss: 2.4025955200195312\n",
      "Time: 329m 16s (- -329m 2s), Epoch: [1/3], Step: [1041/6668], Train Loss: 1.9097064208984376\n",
      "Time: 329m 34s (- -330m 44s), Epoch: [1/3], Step: [1042/6668], Train Loss: 1.6605052185058593\n",
      "Time: 329m 51s (- -330m 27s), Epoch: [1/3], Step: [1043/6668], Train Loss: 2.0555624389648437\n",
      "Time: 330m 11s (- -330m 7s), Epoch: [1/3], Step: [1044/6668], Train Loss: 2.407033233642578\n",
      "Time: 330m 28s (- -331m 50s), Epoch: [1/3], Step: [1045/6668], Train Loss: 2.1857843017578125\n",
      "Time: 330m 46s (- -331m 32s), Epoch: [1/3], Step: [1046/6668], Train Loss: 2.186992492675781\n",
      "Time: 331m 5s (- -331m 13s), Epoch: [1/3], Step: [1047/6668], Train Loss: 2.234113922119141\n",
      "Time: 331m 23s (- -332m 55s), Epoch: [1/3], Step: [1048/6668], Train Loss: 2.1171029663085936\n",
      "Time: 331m 42s (- -332m 36s), Epoch: [1/3], Step: [1049/6668], Train Loss: 1.1401810455322265\n",
      "Time: 332m 1s (- -332m 17s), Epoch: [1/3], Step: [1050/6668], Train Loss: 2.322171936035156\n",
      "Time: 332m 21s (- -333m 57s), Epoch: [1/3], Step: [1051/6668], Train Loss: 1.946611328125\n",
      "Time: 332m 38s (- -333m 40s), Epoch: [1/3], Step: [1052/6668], Train Loss: 2.0634410095214846\n",
      "Time: 332m 56s (- -333m 22s), Epoch: [1/3], Step: [1053/6668], Train Loss: 1.9727247619628907\n",
      "Time: 333m 13s (- -333m 5s), Epoch: [1/3], Step: [1054/6668], Train Loss: 2.2914617919921874\n",
      "Time: 333m 31s (- -334m 47s), Epoch: [1/3], Step: [1055/6668], Train Loss: 2.483303680419922\n",
      "Time: 333m 50s (- -334m 28s), Epoch: [1/3], Step: [1056/6668], Train Loss: 1.8006558227539062\n",
      "Time: 334m 8s (- -334m 10s), Epoch: [1/3], Step: [1057/6668], Train Loss: 1.8141844177246094\n",
      "Time: 334m 27s (- -335m 51s), Epoch: [1/3], Step: [1058/6668], Train Loss: 1.630876007080078\n",
      "Time: 334m 44s (- -335m 34s), Epoch: [1/3], Step: [1059/6668], Train Loss: 1.8013174438476562\n",
      "Time: 335m 2s (- -335m 16s), Epoch: [1/3], Step: [1060/6668], Train Loss: 1.8512106323242188\n",
      "Time: 335m 19s (- -336m 59s), Epoch: [1/3], Step: [1061/6668], Train Loss: 1.763960723876953\n",
      "Time: 335m 37s (- -336m 41s), Epoch: [1/3], Step: [1062/6668], Train Loss: 1.8750416564941406\n",
      "Time: 335m 55s (- -336m 23s), Epoch: [1/3], Step: [1063/6668], Train Loss: 2.140894775390625\n",
      "Time: 336m 12s (- -336m 6s), Epoch: [1/3], Step: [1064/6668], Train Loss: 2.0329159545898436\n",
      "Time: 336m 30s (- -337m 48s), Epoch: [1/3], Step: [1065/6668], Train Loss: 1.6370059204101564\n",
      "Time: 336m 49s (- -337m 29s), Epoch: [1/3], Step: [1066/6668], Train Loss: 1.6044979858398438\n",
      "Time: 337m 7s (- -337m 11s), Epoch: [1/3], Step: [1067/6668], Train Loss: 1.8718765258789063\n",
      "Time: 337m 24s (- -338m 54s), Epoch: [1/3], Step: [1068/6668], Train Loss: 2.099750671386719\n",
      "Time: 337m 42s (- -338m 36s), Epoch: [1/3], Step: [1069/6668], Train Loss: 1.8759243774414063\n",
      "Time: 338m 0s (- -338m 18s), Epoch: [1/3], Step: [1070/6668], Train Loss: 1.967135467529297\n",
      "Time: 338m 19s (- -339m 59s), Epoch: [1/3], Step: [1071/6668], Train Loss: 1.80814453125\n",
      "Time: 338m 38s (- -339m 40s), Epoch: [1/3], Step: [1072/6668], Train Loss: 2.2565623474121095\n",
      "Time: 338m 57s (- -339m 21s), Epoch: [1/3], Step: [1073/6668], Train Loss: 1.555955047607422\n",
      "Time: 339m 16s (- -339m 2s), Epoch: [1/3], Step: [1074/6668], Train Loss: 1.6244412231445313\n",
      "Time: 339m 34s (- -340m 44s), Epoch: [1/3], Step: [1075/6668], Train Loss: 1.6333888244628907\n",
      "Time: 339m 51s (- -340m 27s), Epoch: [1/3], Step: [1076/6668], Train Loss: 1.7710182189941406\n",
      "Time: 340m 12s (- -340m 6s), Epoch: [1/3], Step: [1077/6668], Train Loss: 2.122777099609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 340m 31s (- -341m 47s), Epoch: [1/3], Step: [1078/6668], Train Loss: 2.4685678100585937\n",
      "Time: 340m 49s (- -341m 29s), Epoch: [1/3], Step: [1079/6668], Train Loss: 1.925594024658203\n",
      "Time: 341m 6s (- -341m 12s), Epoch: [1/3], Step: [1080/6668], Train Loss: 1.9351699829101563\n",
      "Time: 341m 24s (- -342m 54s), Epoch: [1/3], Step: [1081/6668], Train Loss: 1.7206259155273438\n",
      "Time: 341m 41s (- -342m 37s), Epoch: [1/3], Step: [1082/6668], Train Loss: 1.3568260192871093\n",
      "Time: 342m 0s (- -342m 18s), Epoch: [1/3], Step: [1083/6668], Train Loss: 2.043794860839844\n",
      "Time: 342m 17s (- -342m 1s), Epoch: [1/3], Step: [1084/6668], Train Loss: 1.8791023254394532\n",
      "Time: 342m 37s (- -343m 41s), Epoch: [1/3], Step: [1085/6668], Train Loss: 1.7465916442871094\n",
      "Time: 342m 56s (- -343m 22s), Epoch: [1/3], Step: [1086/6668], Train Loss: 1.6403182983398437\n",
      "Time: 343m 13s (- -343m 5s), Epoch: [1/3], Step: [1087/6668], Train Loss: 2.3492414855957033\n",
      "Time: 343m 31s (- -344m 47s), Epoch: [1/3], Step: [1088/6668], Train Loss: 2.3097686767578125\n",
      "Time: 343m 48s (- -344m 30s), Epoch: [1/3], Step: [1089/6668], Train Loss: 2.187106018066406\n",
      "Time: 344m 6s (- -344m 12s), Epoch: [1/3], Step: [1090/6668], Train Loss: 2.0878448486328125\n",
      "Time: 344m 25s (- -345m 53s), Epoch: [1/3], Step: [1091/6668], Train Loss: 1.540442657470703\n",
      "Time: 344m 44s (- -345m 34s), Epoch: [1/3], Step: [1092/6668], Train Loss: 2.240525207519531\n",
      "Time: 345m 3s (- -345m 15s), Epoch: [1/3], Step: [1093/6668], Train Loss: 2.4252580261230468\n",
      "Time: 345m 22s (- -346m 55s), Epoch: [1/3], Step: [1094/6668], Train Loss: 1.7759506225585937\n",
      "Time: 345m 43s (- -346m 35s), Epoch: [1/3], Step: [1095/6668], Train Loss: 1.940088653564453\n",
      "Time: 346m 3s (- -346m 15s), Epoch: [1/3], Step: [1096/6668], Train Loss: 1.7681488037109374\n",
      "Time: 346m 22s (- -347m 56s), Epoch: [1/3], Step: [1097/6668], Train Loss: 2.171204376220703\n",
      "Time: 346m 41s (- -347m 37s), Epoch: [1/3], Step: [1098/6668], Train Loss: 2.0534184265136717\n",
      "Time: 347m 1s (- -347m 17s), Epoch: [1/3], Step: [1099/6668], Train Loss: 1.8332632446289063\n",
      "Time: 347m 20s (- -348m 58s), Epoch: [1/3], Step: [1100/6668], Train Loss: 2.259241485595703\n",
      "Time: 347m 38s (- -348m 40s), Epoch: [1/3], Step: [1101/6668], Train Loss: 1.9976997375488281\n",
      "Time: 347m 57s (- -348m 21s), Epoch: [1/3], Step: [1102/6668], Train Loss: 1.8975054931640625\n",
      "Time: 348m 14s (- -348m 4s), Epoch: [1/3], Step: [1103/6668], Train Loss: 1.8972238159179688\n",
      "Time: 348m 32s (- -349m 46s), Epoch: [1/3], Step: [1104/6668], Train Loss: 1.8970979309082032\n",
      "Time: 348m 51s (- -349m 27s), Epoch: [1/3], Step: [1105/6668], Train Loss: 1.620426025390625\n",
      "Time: 349m 10s (- -349m 8s), Epoch: [1/3], Step: [1106/6668], Train Loss: 2.0617921447753904\n",
      "Time: 349m 29s (- -350m 48s), Epoch: [1/3], Step: [1107/6668], Train Loss: 1.7289907836914062\n",
      "Time: 349m 47s (- -350m 31s), Epoch: [1/3], Step: [1108/6668], Train Loss: 1.78669921875\n",
      "Time: 350m 4s (- -350m 14s), Epoch: [1/3], Step: [1109/6668], Train Loss: 1.9689753723144532\n",
      "Time: 350m 24s (- -351m 54s), Epoch: [1/3], Step: [1110/6668], Train Loss: 2.0145297241210938\n",
      "Time: 350m 44s (- -351m 34s), Epoch: [1/3], Step: [1111/6668], Train Loss: 1.8149310302734376\n",
      "Time: 351m 3s (- -351m 15s), Epoch: [1/3], Step: [1112/6668], Train Loss: 1.7032048034667968\n",
      "Time: 351m 20s (- -352m 58s), Epoch: [1/3], Step: [1113/6668], Train Loss: 1.9416716003417969\n",
      "Time: 351m 38s (- -352m 40s), Epoch: [1/3], Step: [1114/6668], Train Loss: 1.9240367126464843\n",
      "Time: 351m 57s (- -352m 21s), Epoch: [1/3], Step: [1115/6668], Train Loss: 2.0519410705566408\n",
      "Time: 352m 16s (- -352m 2s), Epoch: [1/3], Step: [1116/6668], Train Loss: 1.4349884033203124\n",
      "Time: 352m 34s (- -353m 44s), Epoch: [1/3], Step: [1117/6668], Train Loss: 1.8824565124511718\n",
      "Time: 352m 51s (- -353m 27s), Epoch: [1/3], Step: [1118/6668], Train Loss: 1.9536030578613282\n",
      "Time: 353m 9s (- -353m 9s), Epoch: [1/3], Step: [1119/6668], Train Loss: 2.5051495361328127\n",
      "Time: 353m 26s (- -354m 52s), Epoch: [1/3], Step: [1120/6668], Train Loss: 2.3294691467285156\n",
      "Time: 353m 46s (- -354m 32s), Epoch: [1/3], Step: [1121/6668], Train Loss: 1.6035246276855468\n",
      "Time: 354m 5s (- -354m 13s), Epoch: [1/3], Step: [1122/6668], Train Loss: 2.112812957763672\n",
      "Time: 354m 22s (- -355m 56s), Epoch: [1/3], Step: [1123/6668], Train Loss: 1.6860263061523437\n",
      "Time: 354m 42s (- -355m 36s), Epoch: [1/3], Step: [1124/6668], Train Loss: 1.706423797607422\n",
      "Time: 355m 1s (- -355m 16s), Epoch: [1/3], Step: [1125/6668], Train Loss: 2.1939453125\n",
      "Time: 355m 21s (- -356m 57s), Epoch: [1/3], Step: [1126/6668], Train Loss: 1.7553634643554688\n",
      "Time: 355m 38s (- -356m 40s), Epoch: [1/3], Step: [1127/6668], Train Loss: 1.959581298828125\n",
      "Time: 355m 58s (- -356m 20s), Epoch: [1/3], Step: [1128/6668], Train Loss: 1.5911541748046876\n",
      "Time: 356m 18s (- -356m 0s), Epoch: [1/3], Step: [1129/6668], Train Loss: 2.143065185546875\n",
      "Time: 356m 36s (- -357m 42s), Epoch: [1/3], Step: [1130/6668], Train Loss: 1.5843975830078125\n",
      "Time: 356m 53s (- -357m 25s), Epoch: [1/3], Step: [1131/6668], Train Loss: 1.5775106811523438\n",
      "Time: 357m 11s (- -357m 7s), Epoch: [1/3], Step: [1132/6668], Train Loss: 2.1509881591796876\n",
      "Time: 357m 30s (- -358m 47s), Epoch: [1/3], Step: [1133/6668], Train Loss: 2.778627624511719\n",
      "Time: 357m 48s (- -358m 30s), Epoch: [1/3], Step: [1134/6668], Train Loss: 1.606894989013672\n",
      "Time: 358m 6s (- -358m 12s), Epoch: [1/3], Step: [1135/6668], Train Loss: 1.70366943359375\n",
      "Time: 358m 26s (- -359m 52s), Epoch: [1/3], Step: [1136/6668], Train Loss: 2.1961380004882813\n",
      "Time: 358m 45s (- -359m 33s), Epoch: [1/3], Step: [1137/6668], Train Loss: 1.534073944091797\n",
      "Time: 359m 5s (- -359m 13s), Epoch: [1/3], Step: [1138/6668], Train Loss: 1.586092529296875\n",
      "Time: 359m 23s (- -360m 55s), Epoch: [1/3], Step: [1139/6668], Train Loss: 2.3071282958984374\n",
      "Time: 359m 42s (- -360m 36s), Epoch: [1/3], Step: [1140/6668], Train Loss: 1.8263743591308594\n",
      "Time: 359m 59s (- -360m 19s), Epoch: [1/3], Step: [1141/6668], Train Loss: 2.329605712890625\n",
      "Time: 360m 19s (- -361m 59s), Epoch: [1/3], Step: [1142/6668], Train Loss: 1.7735643005371093\n",
      "Time: 360m 38s (- -361m 40s), Epoch: [1/3], Step: [1143/6668], Train Loss: 2.4341798400878907\n",
      "Time: 360m 56s (- -361m 22s), Epoch: [1/3], Step: [1144/6668], Train Loss: 2.117301483154297\n",
      "Time: 361m 14s (- -361m 4s), Epoch: [1/3], Step: [1145/6668], Train Loss: 2.340323181152344\n",
      "Time: 361m 32s (- -362m 46s), Epoch: [1/3], Step: [1146/6668], Train Loss: 1.7525474548339843\n",
      "Time: 361m 49s (- -362m 29s), Epoch: [1/3], Step: [1147/6668], Train Loss: 1.9919094848632812\n",
      "Time: 362m 7s (- -362m 11s), Epoch: [1/3], Step: [1148/6668], Train Loss: 1.8491310119628905\n",
      "Time: 362m 26s (- -363m 51s), Epoch: [1/3], Step: [1149/6668], Train Loss: 1.65205078125\n",
      "Time: 362m 44s (- -363m 34s), Epoch: [1/3], Step: [1150/6668], Train Loss: 2.1146917724609375\n",
      "Time: 363m 4s (- -363m 14s), Epoch: [1/3], Step: [1151/6668], Train Loss: 2.146618194580078\n",
      "Time: 363m 23s (- -364m 55s), Epoch: [1/3], Step: [1152/6668], Train Loss: 2.0317814636230467\n",
      "Time: 363m 43s (- -364m 35s), Epoch: [1/3], Step: [1153/6668], Train Loss: 2.4426473999023437\n",
      "Time: 364m 2s (- -364m 16s), Epoch: [1/3], Step: [1154/6668], Train Loss: 2.0244683837890625\n",
      "Time: 364m 20s (- -365m 58s), Epoch: [1/3], Step: [1155/6668], Train Loss: 2.3484481811523437\n",
      "Time: 364m 39s (- -365m 39s), Epoch: [1/3], Step: [1156/6668], Train Loss: 2.0389555358886717\n",
      "Time: 364m 58s (- -365m 19s), Epoch: [1/3], Step: [1157/6668], Train Loss: 1.8003506469726562\n",
      "Time: 365m 18s (- -365m 0s), Epoch: [1/3], Step: [1158/6668], Train Loss: 1.8843324279785156\n",
      "Time: 365m 38s (- -366m 40s), Epoch: [1/3], Step: [1159/6668], Train Loss: 2.2007479858398438\n",
      "Time: 365m 56s (- -366m 22s), Epoch: [1/3], Step: [1160/6668], Train Loss: 1.6129502868652343\n",
      "Time: 366m 13s (- -366m 5s), Epoch: [1/3], Step: [1161/6668], Train Loss: 1.90569580078125\n",
      "Time: 366m 31s (- -367m 47s), Epoch: [1/3], Step: [1162/6668], Train Loss: 1.983111114501953\n",
      "Time: 366m 50s (- -367m 28s), Epoch: [1/3], Step: [1163/6668], Train Loss: 1.9111628723144531\n",
      "Time: 367m 7s (- -367m 11s), Epoch: [1/3], Step: [1164/6668], Train Loss: 2.035364227294922\n",
      "Time: 367m 27s (- -368m 51s), Epoch: [1/3], Step: [1165/6668], Train Loss: 1.7774510192871094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 367m 46s (- -368m 32s), Epoch: [1/3], Step: [1166/6668], Train Loss: 2.1760015869140625\n",
      "Time: 368m 5s (- -368m 12s), Epoch: [1/3], Step: [1167/6668], Train Loss: 1.7267547607421876\n",
      "Time: 368m 23s (- -369m 55s), Epoch: [1/3], Step: [1168/6668], Train Loss: 1.9412924194335937\n",
      "Time: 368m 41s (- -369m 37s), Epoch: [1/3], Step: [1169/6668], Train Loss: 2.18977783203125\n",
      "Time: 369m 0s (- -369m 18s), Epoch: [1/3], Step: [1170/6668], Train Loss: 1.8739230346679687\n",
      "Time: 369m 19s (- -370m 58s), Epoch: [1/3], Step: [1171/6668], Train Loss: 2.27038330078125\n",
      "Time: 369m 37s (- -370m 41s), Epoch: [1/3], Step: [1172/6668], Train Loss: 2.3267500305175783\n",
      "Time: 369m 55s (- -370m 23s), Epoch: [1/3], Step: [1173/6668], Train Loss: 1.8754090881347656\n",
      "Time: 370m 12s (- -370m 5s), Epoch: [1/3], Step: [1174/6668], Train Loss: 1.8428948974609376\n",
      "Time: 370m 30s (- -371m 48s), Epoch: [1/3], Step: [1175/6668], Train Loss: 1.716855010986328\n",
      "Time: 370m 48s (- -371m 30s), Epoch: [1/3], Step: [1176/6668], Train Loss: 2.0315617370605468\n",
      "Time: 371m 5s (- -371m 12s), Epoch: [1/3], Step: [1177/6668], Train Loss: 2.01703369140625\n",
      "Time: 371m 25s (- -372m 53s), Epoch: [1/3], Step: [1178/6668], Train Loss: 2.143209686279297\n",
      "Time: 371m 44s (- -372m 34s), Epoch: [1/3], Step: [1179/6668], Train Loss: 2.1616827392578126\n",
      "Time: 372m 3s (- -372m 15s), Epoch: [1/3], Step: [1180/6668], Train Loss: 2.1260646057128905\n",
      "Time: 372m 21s (- -373m 57s), Epoch: [1/3], Step: [1181/6668], Train Loss: 1.8848324584960938\n",
      "Time: 372m 40s (- -373m 38s), Epoch: [1/3], Step: [1182/6668], Train Loss: 1.7042567443847656\n",
      "Time: 372m 58s (- -373m 20s), Epoch: [1/3], Step: [1183/6668], Train Loss: 1.7977328491210938\n",
      "Time: 373m 17s (- -373m 1s), Epoch: [1/3], Step: [1184/6668], Train Loss: 2.1891957092285157\n",
      "Time: 373m 36s (- -374m 41s), Epoch: [1/3], Step: [1185/6668], Train Loss: 2.160902252197266\n",
      "Time: 373m 56s (- -374m 22s), Epoch: [1/3], Step: [1186/6668], Train Loss: 1.6048243713378907\n",
      "Time: 374m 16s (- -374m 2s), Epoch: [1/3], Step: [1187/6668], Train Loss: 2.0838719177246094\n",
      "Time: 374m 35s (- -375m 43s), Epoch: [1/3], Step: [1188/6668], Train Loss: 2.148059844970703\n",
      "Time: 374m 54s (- -375m 24s), Epoch: [1/3], Step: [1189/6668], Train Loss: 2.0931852722167967\n",
      "Time: 375m 12s (- -375m 6s), Epoch: [1/3], Step: [1190/6668], Train Loss: 1.8786407470703126\n",
      "Time: 375m 31s (- -376m 47s), Epoch: [1/3], Step: [1191/6668], Train Loss: 2.1948361206054687\n",
      "Time: 375m 51s (- -376m 27s), Epoch: [1/3], Step: [1192/6668], Train Loss: 1.6903619384765625\n",
      "Time: 376m 11s (- -376m 7s), Epoch: [1/3], Step: [1193/6668], Train Loss: 1.8163432312011718\n",
      "Time: 376m 30s (- -377m 48s), Epoch: [1/3], Step: [1194/6668], Train Loss: 1.8515643310546874\n",
      "Time: 376m 49s (- -377m 29s), Epoch: [1/3], Step: [1195/6668], Train Loss: 1.786937255859375\n",
      "Time: 377m 7s (- -377m 11s), Epoch: [1/3], Step: [1196/6668], Train Loss: 1.7496739196777344\n",
      "Time: 377m 25s (- -378m 53s), Epoch: [1/3], Step: [1197/6668], Train Loss: 2.0463714599609375\n",
      "Time: 377m 42s (- -378m 36s), Epoch: [1/3], Step: [1198/6668], Train Loss: 1.8717658996582032\n",
      "Time: 378m 1s (- -378m 16s), Epoch: [1/3], Step: [1199/6668], Train Loss: 2.1163978576660156\n",
      "Time: 378m 19s (- -379m 59s), Epoch: [1/3], Step: [1200/6668], Train Loss: 2.1776634216308595\n",
      "Time: 378m 36s (- -379m 41s), Epoch: [1/3], Step: [1201/6668], Train Loss: 1.7622833251953125\n",
      "Time: 378m 56s (- -379m 22s), Epoch: [1/3], Step: [1202/6668], Train Loss: 1.936256103515625\n",
      "Time: 379m 14s (- -379m 4s), Epoch: [1/3], Step: [1203/6668], Train Loss: 2.4621656799316405\n",
      "Time: 379m 33s (- -380m 45s), Epoch: [1/3], Step: [1204/6668], Train Loss: 2.3480070495605467\n",
      "Time: 379m 51s (- -380m 27s), Epoch: [1/3], Step: [1205/6668], Train Loss: 1.8089910888671874\n",
      "Time: 380m 8s (- -380m 10s), Epoch: [1/3], Step: [1206/6668], Train Loss: 2.4508207702636717\n",
      "Time: 380m 28s (- -381m 50s), Epoch: [1/3], Step: [1207/6668], Train Loss: 1.80408447265625\n",
      "Time: 380m 48s (- -381m 30s), Epoch: [1/3], Step: [1208/6668], Train Loss: 1.8456935119628906\n",
      "Time: 381m 5s (- -381m 13s), Epoch: [1/3], Step: [1209/6668], Train Loss: 2.71224609375\n",
      "Time: 381m 23s (- -382m 55s), Epoch: [1/3], Step: [1210/6668], Train Loss: 2.030348663330078\n",
      "Time: 381m 43s (- -382m 35s), Epoch: [1/3], Step: [1211/6668], Train Loss: 2.1900140380859376\n",
      "Time: 382m 0s (- -382m 18s), Epoch: [1/3], Step: [1212/6668], Train Loss: 2.2192291259765624\n",
      "Time: 382m 20s (- -383m 58s), Epoch: [1/3], Step: [1213/6668], Train Loss: 1.7610397338867188\n",
      "Time: 382m 37s (- -383m 41s), Epoch: [1/3], Step: [1214/6668], Train Loss: 1.7147872924804688\n",
      "Time: 382m 56s (- -383m 22s), Epoch: [1/3], Step: [1215/6668], Train Loss: 2.126387023925781\n",
      "Time: 383m 14s (- -383m 4s), Epoch: [1/3], Step: [1216/6668], Train Loss: 2.2502691650390627\n",
      "Time: 383m 35s (- -384m 43s), Epoch: [1/3], Step: [1217/6668], Train Loss: 1.6826185607910156\n",
      "Time: 383m 55s (- -384m 23s), Epoch: [1/3], Step: [1218/6668], Train Loss: 1.5522593688964843\n",
      "Time: 384m 14s (- -384m 4s), Epoch: [1/3], Step: [1219/6668], Train Loss: 2.243039093017578\n",
      "Time: 384m 32s (- -385m 46s), Epoch: [1/3], Step: [1220/6668], Train Loss: 2.1690664672851563\n",
      "Time: 384m 51s (- -385m 27s), Epoch: [1/3], Step: [1221/6668], Train Loss: 1.9209214782714843\n",
      "Time: 385m 9s (- -385m 9s), Epoch: [1/3], Step: [1222/6668], Train Loss: 1.6392672729492188\n",
      "Time: 385m 29s (- -386m 49s), Epoch: [1/3], Step: [1223/6668], Train Loss: 1.859969024658203\n",
      "Time: 385m 47s (- -386m 31s), Epoch: [1/3], Step: [1224/6668], Train Loss: 2.224282989501953\n",
      "Time: 386m 7s (- -386m 11s), Epoch: [1/3], Step: [1225/6668], Train Loss: 2.3007427978515627\n",
      "Time: 386m 26s (- -387m 52s), Epoch: [1/3], Step: [1226/6668], Train Loss: 1.954000244140625\n",
      "Time: 386m 44s (- -387m 34s), Epoch: [1/3], Step: [1227/6668], Train Loss: 1.8128709411621093\n",
      "Time: 387m 1s (- -387m 17s), Epoch: [1/3], Step: [1228/6668], Train Loss: 2.233290252685547\n",
      "Time: 387m 18s (- -387m 0s), Epoch: [1/3], Step: [1229/6668], Train Loss: 1.4393995666503907\n",
      "Time: 387m 38s (- -388m 40s), Epoch: [1/3], Step: [1230/6668], Train Loss: 1.8085714721679687\n",
      "Time: 387m 57s (- -388m 21s), Epoch: [1/3], Step: [1231/6668], Train Loss: 2.3363336181640624\n",
      "Time: 388m 17s (- -388m 1s), Epoch: [1/3], Step: [1232/6668], Train Loss: 2.038893127441406\n",
      "Time: 388m 38s (- -389m 40s), Epoch: [1/3], Step: [1233/6668], Train Loss: 1.5415544128417968\n",
      "Time: 388m 55s (- -389m 23s), Epoch: [1/3], Step: [1234/6668], Train Loss: 2.005078887939453\n",
      "Time: 389m 15s (- -389m 3s), Epoch: [1/3], Step: [1235/6668], Train Loss: 1.8957200622558594\n",
      "Time: 389m 33s (- -390m 45s), Epoch: [1/3], Step: [1236/6668], Train Loss: 1.6859342956542969\n",
      "Time: 389m 50s (- -390m 28s), Epoch: [1/3], Step: [1237/6668], Train Loss: 2.1587213134765624\n",
      "Time: 390m 8s (- -390m 10s), Epoch: [1/3], Step: [1238/6668], Train Loss: 2.032059326171875\n",
      "Time: 390m 25s (- -391m 53s), Epoch: [1/3], Step: [1239/6668], Train Loss: 1.6207774353027344\n",
      "Time: 390m 43s (- -391m 35s), Epoch: [1/3], Step: [1240/6668], Train Loss: 1.9452984619140625\n",
      "Time: 391m 2s (- -391m 15s), Epoch: [1/3], Step: [1241/6668], Train Loss: 1.9367335510253907\n",
      "Time: 391m 22s (- -392m 56s), Epoch: [1/3], Step: [1242/6668], Train Loss: 2.084416809082031\n",
      "Time: 391m 40s (- -392m 38s), Epoch: [1/3], Step: [1243/6668], Train Loss: 2.009671630859375\n",
      "Time: 391m 57s (- -392m 21s), Epoch: [1/3], Step: [1244/6668], Train Loss: 1.8281573486328124\n",
      "Time: 392m 18s (- -392m 0s), Epoch: [1/3], Step: [1245/6668], Train Loss: 2.048473358154297\n",
      "Time: 392m 37s (- -393m 41s), Epoch: [1/3], Step: [1246/6668], Train Loss: 1.8275186157226562\n",
      "Time: 392m 56s (- -393m 22s), Epoch: [1/3], Step: [1247/6668], Train Loss: 2.168282470703125\n",
      "Time: 393m 16s (- -393m 2s), Epoch: [1/3], Step: [1248/6668], Train Loss: 2.2293440246582032\n",
      "Time: 393m 34s (- -394m 44s), Epoch: [1/3], Step: [1249/6668], Train Loss: 2.2872247314453125\n",
      "Time: 393m 53s (- -394m 25s), Epoch: [1/3], Step: [1250/6668], Train Loss: 1.6038690185546876\n",
      "Time: 394m 11s (- -394m 7s), Epoch: [1/3], Step: [1251/6668], Train Loss: 2.221945037841797\n",
      "Time: 394m 30s (- -395m 48s), Epoch: [1/3], Step: [1252/6668], Train Loss: 2.2238267517089843\n",
      "Time: 394m 48s (- -395m 30s), Epoch: [1/3], Step: [1253/6668], Train Loss: 1.8690451049804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 395m 5s (- -395m 13s), Epoch: [1/3], Step: [1254/6668], Train Loss: 1.9827784729003906\n",
      "Time: 395m 23s (- -396m 55s), Epoch: [1/3], Step: [1255/6668], Train Loss: 1.893043212890625\n",
      "Time: 395m 41s (- -396m 37s), Epoch: [1/3], Step: [1256/6668], Train Loss: 2.0154571533203125\n",
      "Time: 396m 1s (- -396m 17s), Epoch: [1/3], Step: [1257/6668], Train Loss: 2.087542724609375\n",
      "Time: 396m 20s (- -397m 58s), Epoch: [1/3], Step: [1258/6668], Train Loss: 2.2571832275390626\n",
      "Time: 396m 38s (- -397m 40s), Epoch: [1/3], Step: [1259/6668], Train Loss: 1.8148146057128907\n",
      "Time: 396m 56s (- -397m 22s), Epoch: [1/3], Step: [1260/6668], Train Loss: 1.8611041259765626\n",
      "Time: 397m 15s (- -397m 3s), Epoch: [1/3], Step: [1261/6668], Train Loss: 2.089020080566406\n",
      "Time: 397m 35s (- -398m 43s), Epoch: [1/3], Step: [1262/6668], Train Loss: 1.7881573486328124\n",
      "Time: 397m 53s (- -398m 25s), Epoch: [1/3], Step: [1263/6668], Train Loss: 1.6455863952636718\n",
      "Time: 398m 10s (- -398m 8s), Epoch: [1/3], Step: [1264/6668], Train Loss: 1.9983065795898438\n",
      "Time: 398m 28s (- -399m 50s), Epoch: [1/3], Step: [1265/6668], Train Loss: 1.9370413208007813\n",
      "Time: 398m 47s (- -399m 31s), Epoch: [1/3], Step: [1266/6668], Train Loss: 2.205891571044922\n",
      "Time: 399m 5s (- -399m 13s), Epoch: [1/3], Step: [1267/6668], Train Loss: 1.8694009399414062\n",
      "Time: 399m 23s (- -400m 55s), Epoch: [1/3], Step: [1268/6668], Train Loss: 2.112390899658203\n",
      "Time: 399m 40s (- -400m 38s), Epoch: [1/3], Step: [1269/6668], Train Loss: 1.804977264404297\n",
      "Time: 399m 58s (- -400m 20s), Epoch: [1/3], Step: [1270/6668], Train Loss: 1.558599090576172\n",
      "Time: 400m 15s (- -400m 3s), Epoch: [1/3], Step: [1271/6668], Train Loss: 1.8436878967285155\n",
      "Time: 400m 33s (- -401m 45s), Epoch: [1/3], Step: [1272/6668], Train Loss: 2.0268798828125\n",
      "Time: 400m 51s (- -401m 27s), Epoch: [1/3], Step: [1273/6668], Train Loss: 1.7352900695800781\n",
      "Time: 401m 11s (- -401m 7s), Epoch: [1/3], Step: [1274/6668], Train Loss: 1.8208349609375\n",
      "Time: 401m 30s (- -402m 48s), Epoch: [1/3], Step: [1275/6668], Train Loss: 2.246634216308594\n",
      "Time: 401m 48s (- -402m 30s), Epoch: [1/3], Step: [1276/6668], Train Loss: 1.83348388671875\n",
      "Time: 402m 7s (- -402m 11s), Epoch: [1/3], Step: [1277/6668], Train Loss: 2.139567565917969\n",
      "Time: 402m 29s (- -403m 49s), Epoch: [1/3], Step: [1278/6668], Train Loss: 2.069738006591797\n",
      "Time: 402m 46s (- -403m 32s), Epoch: [1/3], Step: [1279/6668], Train Loss: 1.7490919494628907\n",
      "Time: 403m 6s (- -403m 12s), Epoch: [1/3], Step: [1280/6668], Train Loss: 1.6220790100097657\n",
      "Time: 403m 26s (- -404m 52s), Epoch: [1/3], Step: [1281/6668], Train Loss: 1.6381025695800782\n",
      "Time: 403m 45s (- -404m 33s), Epoch: [1/3], Step: [1282/6668], Train Loss: 2.385548095703125\n",
      "Time: 404m 5s (- -404m 13s), Epoch: [1/3], Step: [1283/6668], Train Loss: 2.116243438720703\n",
      "Time: 404m 22s (- -405m 56s), Epoch: [1/3], Step: [1284/6668], Train Loss: 1.9845123291015625\n",
      "Time: 404m 40s (- -405m 38s), Epoch: [1/3], Step: [1285/6668], Train Loss: 2.156361389160156\n",
      "Time: 404m 57s (- -405m 21s), Epoch: [1/3], Step: [1286/6668], Train Loss: 1.858170166015625\n",
      "Time: 405m 15s (- -405m 3s), Epoch: [1/3], Step: [1287/6668], Train Loss: 2.051533203125\n",
      "Time: 405m 35s (- -406m 43s), Epoch: [1/3], Step: [1288/6668], Train Loss: 2.0483407592773437\n",
      "Time: 405m 53s (- -406m 25s), Epoch: [1/3], Step: [1289/6668], Train Loss: 1.7747779846191407\n",
      "Time: 406m 12s (- -406m 6s), Epoch: [1/3], Step: [1290/6668], Train Loss: 2.4051104736328126\n",
      "Time: 406m 31s (- -407m 47s), Epoch: [1/3], Step: [1291/6668], Train Loss: 1.8825274658203126\n",
      "Time: 406m 49s (- -407m 29s), Epoch: [1/3], Step: [1292/6668], Train Loss: 1.7589913940429687\n",
      "Time: 407m 8s (- -407m 10s), Epoch: [1/3], Step: [1293/6668], Train Loss: 1.9042549133300781\n",
      "Time: 407m 26s (- -408m 52s), Epoch: [1/3], Step: [1294/6668], Train Loss: 1.753190155029297\n",
      "Time: 407m 44s (- -408m 34s), Epoch: [1/3], Step: [1295/6668], Train Loss: 2.0783041381835936\n",
      "Time: 408m 1s (- -408m 17s), Epoch: [1/3], Step: [1296/6668], Train Loss: 2.230424652099609\n",
      "Time: 408m 19s (- -409m 59s), Epoch: [1/3], Step: [1297/6668], Train Loss: 1.9058050537109374\n",
      "Time: 408m 37s (- -409m 41s), Epoch: [1/3], Step: [1298/6668], Train Loss: 1.7737251281738282\n",
      "Time: 408m 56s (- -409m 22s), Epoch: [1/3], Step: [1299/6668], Train Loss: 1.5071629333496093\n",
      "Time: 409m 14s (- -409m 4s), Epoch: [1/3], Step: [1300/6668], Train Loss: 2.4426890563964845\n",
      "Time: 409m 31s (- -410m 46s), Epoch: [1/3], Step: [1301/6668], Train Loss: 2.577249755859375\n",
      "Time: 409m 51s (- -410m 27s), Epoch: [1/3], Step: [1302/6668], Train Loss: 1.5286192321777343\n",
      "Time: 410m 8s (- -410m 10s), Epoch: [1/3], Step: [1303/6668], Train Loss: 1.8547689819335937\n",
      "Time: 410m 26s (- -411m 52s), Epoch: [1/3], Step: [1304/6668], Train Loss: 2.1733502197265624\n",
      "Time: 410m 45s (- -411m 33s), Epoch: [1/3], Step: [1305/6668], Train Loss: 1.8894242858886718\n",
      "Time: 411m 5s (- -411m 13s), Epoch: [1/3], Step: [1306/6668], Train Loss: 2.01714599609375\n",
      "Time: 411m 23s (- -412m 55s), Epoch: [1/3], Step: [1307/6668], Train Loss: 1.930121612548828\n",
      "Time: 411m 42s (- -412m 36s), Epoch: [1/3], Step: [1308/6668], Train Loss: 1.6404646301269532\n",
      "Time: 412m 1s (- -412m 17s), Epoch: [1/3], Step: [1309/6668], Train Loss: 2.101562805175781\n",
      "Time: 412m 20s (- -413m 57s), Epoch: [1/3], Step: [1310/6668], Train Loss: 1.955194091796875\n",
      "Time: 412m 40s (- -413m 38s), Epoch: [1/3], Step: [1311/6668], Train Loss: 2.09963134765625\n",
      "Time: 412m 58s (- -413m 20s), Epoch: [1/3], Step: [1312/6668], Train Loss: 2.754993896484375\n",
      "Time: 413m 15s (- -413m 3s), Epoch: [1/3], Step: [1313/6668], Train Loss: 1.9314364624023437\n",
      "Time: 413m 34s (- -414m 44s), Epoch: [1/3], Step: [1314/6668], Train Loss: 1.9494522094726563\n",
      "Time: 413m 54s (- -414m 24s), Epoch: [1/3], Step: [1315/6668], Train Loss: 1.6878616333007812\n",
      "Time: 414m 13s (- -414m 4s), Epoch: [1/3], Step: [1316/6668], Train Loss: 1.5621846008300782\n",
      "Time: 414m 33s (- -415m 45s), Epoch: [1/3], Step: [1317/6668], Train Loss: 2.2187760925292968\n",
      "Time: 414m 52s (- -415m 26s), Epoch: [1/3], Step: [1318/6668], Train Loss: 2.0308549499511717\n",
      "Time: 415m 12s (- -415m 6s), Epoch: [1/3], Step: [1319/6668], Train Loss: 1.8861679077148437\n",
      "Time: 415m 32s (- -416m 46s), Epoch: [1/3], Step: [1320/6668], Train Loss: 1.6707752990722655\n",
      "Time: 415m 49s (- -416m 29s), Epoch: [1/3], Step: [1321/6668], Train Loss: 1.7060707092285157\n",
      "Time: 416m 7s (- -416m 11s), Epoch: [1/3], Step: [1322/6668], Train Loss: 2.190714569091797\n",
      "Time: 416m 24s (- -417m 54s), Epoch: [1/3], Step: [1323/6668], Train Loss: 2.020604553222656\n",
      "Time: 416m 44s (- -417m 34s), Epoch: [1/3], Step: [1324/6668], Train Loss: 1.7110598754882813\n",
      "Time: 417m 3s (- -417m 15s), Epoch: [1/3], Step: [1325/6668], Train Loss: 1.996942138671875\n",
      "Time: 417m 23s (- -418m 55s), Epoch: [1/3], Step: [1326/6668], Train Loss: 2.2633456420898437\n",
      "Time: 417m 40s (- -418m 37s), Epoch: [1/3], Step: [1327/6668], Train Loss: 1.7317572021484375\n",
      "Time: 417m 58s (- -418m 20s), Epoch: [1/3], Step: [1328/6668], Train Loss: 2.0856112670898437\n",
      "Time: 418m 15s (- -418m 2s), Epoch: [1/3], Step: [1329/6668], Train Loss: 1.8727049255371093\n",
      "Time: 418m 36s (- -419m 41s), Epoch: [1/3], Step: [1330/6668], Train Loss: 1.8301031494140625\n",
      "Time: 418m 56s (- -419m 22s), Epoch: [1/3], Step: [1331/6668], Train Loss: 2.0978578186035155\n",
      "Time: 419m 16s (- -419m 2s), Epoch: [1/3], Step: [1332/6668], Train Loss: 1.7159873962402343\n",
      "Time: 419m 34s (- -420m 44s), Epoch: [1/3], Step: [1333/6668], Train Loss: 2.0082138061523436\n",
      "Time: 419m 51s (- -420m 27s), Epoch: [1/3], Step: [1334/6668], Train Loss: 1.9136929321289062\n",
      "Time: 420m 9s (- -420m 9s), Epoch: [1/3], Step: [1335/6668], Train Loss: 1.8287928771972657\n",
      "Time: 420m 28s (- -421m 49s), Epoch: [1/3], Step: [1336/6668], Train Loss: 2.1698454284667967\n",
      "Time: 420m 48s (- -421m 30s), Epoch: [1/3], Step: [1337/6668], Train Loss: 2.237202453613281\n",
      "Time: 421m 13s (- -421m 5s), Epoch: [1/3], Step: [1338/6668], Train Loss: 1.9988870239257812\n",
      "Time: 421m 44s (- -422m 33s), Epoch: [1/3], Step: [1339/6668], Train Loss: 2.0136141967773438\n",
      "Time: 422m 7s (- -422m 11s), Epoch: [1/3], Step: [1340/6668], Train Loss: 1.891668701171875\n",
      "Time: 422m 30s (- -423m 48s), Epoch: [1/3], Step: [1341/6668], Train Loss: 1.9900177001953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 422m 56s (- -423m 22s), Epoch: [1/3], Step: [1342/6668], Train Loss: 2.027724609375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-471-89085091c904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saved_model/encoder_hiddenSize{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-468-67fc97b13225>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 100 * 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 17\u001b[0;31m                          decoder, encoder_optimizer, decoder_optimizer, criterion, mask = mask)\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-467-340210bf27ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# encoder_outputs: torch.Size([100, 32, 512])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 52\u001b[0;31m                 decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# topv: 32*1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-466-3e7432d50ff1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m#         output: torch.Size([32, 69126])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#         hidden: torch.Size([1, 32, 256])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(input_lang.n_words,hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 3, print_every=1,plot_every=1)\n",
    "\n",
    "torch.save(encoder1.state_dict(), \"saved_model/encoder_hiddenSize{}\".format(hidden_size))\n",
    "torch.save(attn_decoder1.state_dict(), \"saved_model/attn_decoder_hiddenSize{}\".format(hidden_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
